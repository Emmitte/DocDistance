神经网络发展回顾 自从西班牙解剖学家Ｃａｊａｌ于１９世纪末创立 了神经元学说以来，关于神经元的生物学特征和相 关的电学性质在之后被相继发现．１９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ．
图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数．神经元 ８期焦李成等：神经网络七十年：回顾与展望１６９９ 的输出ｙｉ 可以表示为如下形式： ｙｉ＝ｆ Σ ｎ ｊ＝１ ｗｉｊｘｊ－（ θｉ） （１） 　　该模型从逻辑功能器件的角度来描述神经元， 为神经网络的理论研究开辟了道路．Ｍ－Ｐ模型是对 生物神经元信息处理模式的数学简化，后续的神经 网络研究工作都是以它为基础的．１９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］．受启发于巴浦洛夫的条件反 射实验，Ｈｅｂｂ认为如果两个神经元在同一时刻被 激发，则它们之间的联系应该被强化，基于此所定义 的Ｈｅｂｂ学习规则如下所示： ｗｉｊ（ｔ＋１）＝ｗｉｊ（ｔ）＋αｙｊ（ｔ）ｙｉ（ｔ） （２） 其中，ｗｉｊ（ｔ＋１）和ｗｉｊ（ｔ）分别表示在ｔ＋１和ｔ 时刻时，神经元ｊ到神经元ｉ之间的连接强度，而ｙｉ 和ｙｊ 则为神经元ｉ和ｊ 的输出．
Ｈｅｂｂ规则隶属于无监督学习算法的范畴，其 主要思想是根据两个神经元的激发状态来调整其连 接关系，以此实现对简单神经活动的模拟．继Ｈｅｂｂ 学习规则之后，神经元的有监督Ｄｅｌｔａ学习规则被 提出，用以解决在输入输出已知的情况下神经元权 值的学习问题．该算法通过对连接权值进行不断调 整以使神经元的实际输出和期望输出到达一致，其 学习修正公式如下［２１］： ｗｉｊ（ｔ＋１）＝ｗｉｊ（ｔ）＋α（ｄｉ－ｙｉ）ｘｊ（ｔ） （３） 其中，α为算法的学习速率，ｄｉ 和ｙｉ 为神经元ｉ 的期望输出和实际输出，ｘｊ（ｔ）表示神经元ｊ在ｔ 时 刻的状态（激活或抑制）．
从直观上来说，当神经元ｉ的实际输出比期望 输出大，则减小与已激活神经元的连接权重，同时增 加与已抑制神经元的连接权重；当神经元ｉ的实际 输出比期望输出小，则增加与已激活神经元的连接 权重，同时减小与已抑制神经元的连接权重．通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力．Ｈｅｂｂ学习规则和Ｄｅｌｔａ学习规则都是针对单 个神经元而提出的，在神经元组成的网络中参数的 学习规则将会在后续述及．以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究．
１９５８年，Ｒｏｓｅｎｂｌａｔｔ等人成功研制出了代号为 Ｍａｒｋ　Ｉ的感知机（Ｐｅｒｃｅｐｔｒｏｎ），这是历史上首个将 神经网络的学习功能用于模式识别的装置，标志着 神经网路进入了新的发展阶段［２２］．感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测． 假设输入特征向量空间为ｘ∈!ｎ，输出类标空间为 ｙ＝｛－１，＋１｝，则感知机模型如下： ｙ＝ｆ（ｘ）＝ｓｉｇｎ（ｗ·ｘ＋ｂ） （４） 其中，ｗ 和ｂ 为神经元的权值向量和偏置；ｗ·ｘ 表 示ｗ 和ｘ 的内积；ｓｉｇｎ为符号函数： ｓｉｇｎ（ｘ）＝ ＋１， ｘ０ ｛－１， ｘ＜０ （５） 
感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类．感知机参数的学 习是基于经验损失函数最小化的，旨在最小化误分类 点到决策平面的距离．给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降．首先分别求出损失函 数对ｗ 和ｂ 偏导数： ｗＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｘｉ·ｙｉ （８） ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ （９） 然后，随机选取一个误分类点（ｘｉ，ｙｉ）对ｗ 和ｂ 进行更新 ｗｎｅｗ＝ｗｏｌｄ＋ηｙｉｘｉ （１０） ｂｎｅｗ＝ｂｏｌｄ＋ηｙｉ （１１） 其中，０＜η１是学习步长．以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式．其基本思想是将ｗ 和ｂ 表示为所有 实例点的线性组合形式，通过求解系数来得到ｗ 和 ｂ．不失一般性，首先将ｗ 和ｂ 的初始值设为０，对于 误分类点按照式（１０）和（１１）的规则来对ｗ 和ｂ 的 值进行更新．假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数）．继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］．