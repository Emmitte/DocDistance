神经网络发展回顾 自从西班牙解剖学家Ｃａｊａｌ于１９世纪末创立 了神经元学说以来，关于神经元的生物学特征和相 关的电学性质在之后被相继发现 VS 研究表明,大数据应用具有不同于传统应用的数据局部性特征 = 0.07106690545187015
１９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ VS (2) 具有良好的编程模型和编程接口 = 0.1466471150213533
图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 VS (2) 提高I/O 访问效率 = 0.16514456476895406
神经元 ８期焦李成等：神经网络七十年：回顾与展望１６９９ 的输出ｙｉ 可以表示为如下形式： ｙｉ＝ｆ Σ ｎ ｊ＝１ ｗｉｊｘｊ－（ θｉ） （１） 　　该模型从逻辑功能器件的角度来描述神经元， 为神经网络的理论研究开辟了道路 VS 此外,由于新型非易失性内存的I/O 延迟远小于磁盘,传统模型中磁盘与内存的数据一致性等问题不复存在,编程模型也因此有相应改变 = 0.09284766908852594
Ｍ－Ｐ模型是对 生物神经元信息处理模式的数学简化，后续的神经 网络研究工作都是以它为基础的 VS (4) 网络瓶颈谷歌的一份报告[94]显示:从内存读取数据比从本地磁盘或集群网络中其他主机读取数据快两个数量级,而磁盘和集群网络读取速度又处于同一数量级 = 0.11821656093586509
１９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］ VS 在编程模型方面,研究者提出了以MapReduce[3],Hadoop[4]为代表的编程框架来解决大数据问题 = 0.10286889997472794
受启发于巴浦洛夫的条件反 射实验，Ｈｅｂｂ认为如果两个神经元在同一时刻被 激发，则它们之间的联系应该被强化，基于此所定义 的Ｈｅｂｂ学习规则如下所示： ｗｉｊ（ｔ＋１）＝ｗｉｊ（ｔ）＋αｙｊ（ｔ）ｙｉ（ｔ） （２） 其中，ｗｉｊ（ｔ＋１）和ｗｉｊ（ｔ）分别表示在ｔ＋１和ｔ 时刻时，神经元ｊ到神经元ｉ之间的连接强度，而ｙｉ 和ｙｊ 则为神经元ｉ和ｊ 的输出 VS 因此,在本地处理数据时,数据存在于内存当中,CPU 很少停下来等待磁盘I/O,这样,内存计算解决了磁盘I/O 瓶颈 = 0.08661986608440464
Ｈｅｂｂ规则隶属于无监督学习算法的范畴，其 主要思想是根据两个神经元的激发状态来调整其连 接关系，以此实现对简单神经活动的模拟 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.14142135623730948
继Ｈｅｂｂ 学习规则之后，神经元的有监督Ｄｅｌｔａ学习规则被 提出，用以解决在输入输出已知的情况下神经元权 值的学习问题 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.17616606585441102
该算法通过对连接权值进行不断调 整以使神经元的实际输出和期望输出到达一致，其 学习修正公式如下［２１］： ｗｉｊ（ｔ＋１）＝ｗｉｊ（ｔ）＋α（ｄｉ－ｙｉ）ｘｊ（ｔ） （３） 其中，α为算法的学习速率，ｄｉ 和ｙｉ 为神经元ｉ 的期望输出和实际输出，ｘｊ（ｔ）表示神经元ｊ在ｔ 时 刻的状态（激活或抑制） VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.13884202690012465
从直观上来说，当神经元ｉ的实际输出比期望 输出大，则减小与已激活神经元的连接权重，同时增 加与已抑制神经元的连接权重；当神经元ｉ的实际 输出比期望输出小，则增加与已激活神经元的连接 权重，同时减小与已抑制神经元的连接权重 VS 而大数据所表现出的增量速度快、时间局部性低等特点,客观上使得以计算为中心的传统模式面临着内存容量有限、输入/输出(I/O)压力大、缓存命中率低、数据处理的总体性能低等诸多挑战 = 0.10206207261596574
通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 VS 在内存计算中,数据长久地存储于内存中,由应用程序直接访问 = 0.25197631533948484
Ｈｅｂｂ学习规则和Ｄｅｌｔａ学习规则都是针对单 个神经元而提出的，在神经元组成的网络中参数的 学习规则将会在后续述及 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.1651445647689541
以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 VS 3 Hierarchical hybrid memory structure图3 分层混合内存结构另外,虽然诸如PCM 等新型存储介质和DRAM 构成的混合内存在硬件技术方向的研究已经进展不小,但是其相应的软件平台研究仍相对滞后 = 0.17507524381296344
１９５８年，Ｒｏｓｅｎｂｌａｔｔ等人成功研制出了代号为 Ｍａｒｋ　Ｉ的感知机（Ｐｅｒｃｅｐｔｒｏｎ），这是历史上首个将 神经网络的学习功能用于模式识别的装置，标志着 神经网路进入了新的发展阶段［２２］ VS 例如,一种选择就是将最新产生的数据预取进内存 = 0.06454972243679027
感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测 VS 其次,给出内存计算技术及系统的分类,介绍3 种类别的原理、现状和问题 = 0.19245008972987526
 假设输入特征向量空间为ｘ∈!ｎ，输出类标空间为 ｙ＝｛－１，＋１｝，则感知机模型如下： ｙ＝ｆ（ｘ）＝ｓｉｇｎ（ｗ·ｘ＋ｂ） （４） 其中，ｗ 和ｂ 为神经元的权值向量和偏置；ｗ·ｘ 表 示ｗ 和ｘ 的内积；ｓｉｇｎ为符号函数： ｓｉｇｎ（ｘ）＝ ＋１， ｘ０ ｛－１， ｘ＜０ （５）  VS 因此,对于内存缓存,仍然有很大的提升空间和面临的挑战 = 0.07377111135633174
感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 VS 因此,对于内存缓存,仍然有很大的提升空间和面临的挑战 = 0.1403724812687193
感知机参数的学 习是基于经验损失函数最小化的，旨在最小化误分类 点到决策平面的距离 VS 其次,给出内存计算技术及系统的分类,介绍3 种类别的原理、现状和问题 = 0.07647191129018724
给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 VS 从编程者的角度来看,调试及优化算法比分布式更容易 = 0.10388150415966657
首先分别求出损失函 数对ｗ 和ｂ 偏导数： ｗＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｘｉ·ｙｉ （８） ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ （９） 然后，随机选取一个误分类点（ｘｉ，ｙｉ）对ｗ 和ｂ 进行更新 ｗｎｅｗ＝ｗｏｌｄ＋ηｙｉｘｉ （１０） ｂｎｅｗ＝ｂｏｌｄ＋ηｙｉ （１１） 其中，０＜η１是学习步长 VS 此类内存计算研究包括以下几个关键点 = 0.037267799624996496
以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 VS 这种内存是由PCM 和DRAM 构成,具有层次结构 = 0.09999999999999998
其基本思想是将ｗ 和ｂ 表示为所有 实例点的线性组合形式，通过求解系数来得到ｗ 和 ｂ VS 此类内存计算研究包括以下几个关键点 = 0.09128709291752767
不失一般性，首先将ｗ 和ｂ 的初始值设为０，对于 误分类点按照式（１０）和（１１）的规则来对ｗ 和ｂ 的 值进行更新 VS 目前的一些集群内存存储系统,比如分布式共享内存[89]、键/值存储[41]、内存数据库等,都提供了基于细粒度更新可变状态的接口,这些接口的容错方式就是通过集群中节点间的数据移动和复制,或日志更新集群间各个节点 = 0.08592497251893072
假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） VS (2) 提高I/O 访问效率 = 0.1860521018838127
继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ VS 内存计算不仅仅是把数据驻留内存,还需要对软件体系、计算模型等进行专门的设计[25] = 0.13068205256070964
Avg similar = 0.12372346410009469
