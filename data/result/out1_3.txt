１　引　言实现人工智能是人类长期以来一直追求的梦想 VS 同时,IBM 的solidDB[17]、Oracle 的Exadata X3、微软的SQLServer 2012 也引入了内存计算 = 0.11322770341445959
虽然计算机技术在过去几十年里取得了长足的发展，但是实现真正意义上的机器智能至今仍然困难重重 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.09999999999999998
伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 VS 这种内存是由PCM 和DRAM 构成,具有层次结构 = 0.17888543819998318
如何借助神经科学、脑科学与认知科学的研究成果，研究大脑信息表征、转换机理和学习规则，建立模拟大脑信息处理过程的智能计算模型，最终使机器掌握人类的认知规律，是“类脑智能”的研究目标 VS 此类内存计算研究包括以下几个关键点 = 0.1720618004029213
近年来，类脑智能已成为世界各国研究和角逐的热点 VS 因此,在高性能的计算背景下,内存计算能再次成为工业界和学界研究关注的热点,成为海量数据分析的利器则不足为奇 = 0.17149858514250882
继美国及欧盟各国之后，我国经过两三年筹备的“中国脑科学计划”在２０１５年浮出水面，科技部正在规划“脑科学与类脑研究”的重大专项，北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究，大规模“类脑智能”的研究正蓄势待发 VS 相对于硬件体系结构,在软件方面,这种内存计算的相关研究滞后很多 = 0.20998026278290402
类脑智能是涉及计算科学、认知科学、神经科学与脑科学的交叉前沿方向 VS 在应用层面,内存计算主要用于数据密集型计算的处理,尤其是数据量极大且需要实时分析处理的计算 = 0.18257418583505539
类脑智能的实现离不开大脑神经系统的研究 VS 3 Hierarchical hybrid memory structure图3 分层混合内存结构另外,虽然诸如PCM 等新型存储介质和DRAM 构成的混合内存在硬件技术方向的研究已经进展不小,但是其相应的软件平台研究仍相对滞后 = 0.1403724812687193
众所周知，人脑是由几十多亿个高度互联的神经元组成的复杂生物网络，也是人类分析、联想、记忆和逻辑推理等能力的来源 VS (4) 网络瓶颈谷歌的一份报告[94]显示:从内存读取数据比从本地磁盘或集群网络中其他主机读取数据快两个数量级,而磁盘和集群网络读取速度又处于同一数量级 = 0.11058146711617285
神经元之间通过突触连接以相互传递信息，连接的方式和强度随着学习发生改变，从而将学习到的知识进行存储 VS 在实际的应用实例中,应用类型不同,内存数据安排也不尽相同,如Spark 系统采用“键-值”存储方式,Piccolo 采用内存表格方式及“键-值”两种方式,Pregel 采用了BigTable 的内存组织方式 = 0.15214515486254612
模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 VS 在内存计算中,数据长久地存储于内存中,由应用程序直接访问 = 0.17928429140015903
神经网络的计算结构和学习规则遵照生物神经网络设计，在数字计算机中，神经细胞接收周围细胞的刺激并产生相应输出信号的过程可以用“线性加权和”及“函数映射”的方式来模拟，而网络结构和权值调整的过程用优化学习算法实现 VS 常见的同步方式有同步计算、异步计算和混合方式 = 0.1653796461189446
按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 VS 常见的同步方式有同步计算、异步计算和混合方式 = 0.2666666666666666
从２０世纪４０年代的Ｍ－Ｐ神经元和Ｈｅｂｂ学习规则，到５０年代的Ｈｏｄｙｋｉｎ－Ｈｕｘｌｅｙ方程、感知器模型与自适应滤波器，再到６０年代的自组织映射网络、神经认知机、自适应共振网络，许多神经计算模型都发展成为信号处理、计算机视觉、自然语言处理与优化计算等领域的经典方法，为该领域带来了里程碑式的影响 VS 因此,在分布式集群内存计算当中,网络瓶颈成为主要瓶颈 = 0.14285714285714285
目前神经网络已经发展了上百种模型，在诸如手写体识别［１－２］、图像标注［３］、语义理解［４－６］和语音识别［７－９］等技术领域取得了非常成功的应用 VS (2) 具有良好的编程模型和编程接口 = 0.09128709291752768
从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 VS 这对于对实时性要求很高的内存计算来说是非常耗时的,因为网络数据移动的延迟远远大于本地的数据移动,吞吐量也远远小于本地数据移动 = 0.3572172541558802
例如，人脑被证明可以在没有导师监督的情况下主动地完成学习任务，仅凭借传统的浅层神经网络是无法实现这一点的 VS 大数据所带来的大规模及需要实时处理等特点与传统的以计算为中心的模式产生巨大矛盾,使得传统计算模型难以适应当今大数据环境下的数据处理 = 0.12038585308576923
最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 VS 其次,给出内存计算技术及系统的分类,介绍3 种类别的原理、现状和问题 = 0.18490006540840973
深层神经网络通过增加网络的层数来模拟人脑复杂的层次化认知规律，以使机器获得“抽象概念”的能力，在无监督特征学习方面具有更强的能力 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.1318760946791574
然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 VS 因此,在高性能的计算背景下,内存计算能再次成为工业界和学界研究关注的热点,成为海量数据分析的利器则不足为奇 = 0.242535625036333
２００６年，Ｈｉｎｔｏｎ在《科学》上提出了一种面向复杂通用学习任务的深层神经网络，指出具有大量隐层的网络具有优异的特征学习能力，而网络的训练可以采用“逐层初始化”与“反向微调”技术解决 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.16035674514745463
人类借助神经网络找到了处理“抽象概念”的方法，神经网络的研究又进入了一个崭新的时代［１０－１２］，深度学习的概念开始被提出 VS 1 内存计算概念内存计算不是一个新的概念,早在20 世纪90 年代就有关于内存计算雏形的论述[18,19],只是当时硬件发展有限,没有得到进一步地研究 = 0.11028219331407116
深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 VS 内存计算不是最新提出的概念,但是近年来它却成为业界和研究领域的一个热点,解决了前面提到的大数据时代数据处理速度以及时效性的问题,其原因在于,在内存计算模式下,所有的数据在初始化阶段全部加载到内存中,数据及查询的操作都在高速内存中执行,CPU 直接从内存读取数据,进行实时地计算和分析,减少了磁盘数据访问,降低了网络与磁盘I/O 的影响,大幅提升了计算处理的数据吞吐量与处理的速度,减少了原本占大量计算资源的I/O 开销 = 0.3454246398538788
深层神经网络能够获得巨大成功与其对应在训练算法上所取得的突破性进展是密不可分的 VS 从编程者的角度来看,调试及优化算法比分布式更容易 = 0.13608276348795434
传统的反向传播算法（Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ）随着传递层数的增加，残差会越来越小，出现所谓的“梯度扩散”（Ｇｒａｄｉｅｎｔ　Ｄｉｆｆｕｓｉｏｎ）现象，故而不适于深层网络的训练 VS (4) 网络瓶颈谷歌的一份报告[94]显示:从内存读取数据比从本地磁盘或集群网络中其他主机读取数据快两个数量级,而磁盘和集群网络读取速度又处于同一数量级 = 0.09652341781316803
深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.2595542738092201
除了以上优势外，深度学习最具吸引力的地方还在于能凭借无标签的数据来进行学习，而不需要依赖于监督信息的支撑［１３］ VS 研究表明,大数据应用具有不同于传统应用的数据局部性特征 = 0.15713484026367722
现实世界的很多问题中，对数据的标记通常是耗时耗力甚至是不可行的，无监督学习可以自动抽取出抽象的高层属性和特征，是解决样本标记难问题的一个重大突破 VS 研究表明,大数据应用具有不同于传统应用的数据局部性特征 = 0.19611613513818404
深度学习的成功引起了包括产业界和学术界在内的诸多人士的关注，其影响力甚至上升到了国家战略层面 VS SAP 公司在2012 年推出的HANA内存计算[15]及加州大学伯克利分校开发的ApacheSpark[16]使得内存计算再次得到学术界和工业界的广泛关注 = 0.09901475429766744
２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.19364916731037085
２０１３年１月，作为百度公司创始人兼ＣＥＯ的李彦宏在其年会上宣布了成立百度研究院的计划，并且强调首当其冲的就是组建“深度学习研究所” VS 7 年 = 0.20412414523193154
在２０１５年３月９日的两会期间，李彦宏又提议设立“中国大脑”计划的提案，与２０１３年１月和２０１３年４月的“欧盟大脑计划”和“美国大脑计划”相呼应 = 0.0
２０１５年３月，阿里巴巴公司的创始人马云通过支付宝的“刷脸支付”功能，在德国举行的ＩＴ 博览会上成功购得了一款汉诺威纪念邮票 VS SAP 公司在2012 年推出的HANA内存计算[15]及加州大学伯克利分校开发的ApacheSpark[16]使得内存计算再次得到学术界和工业界的广泛关注 = 0.044543540318737404
这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” VS (1) 内存压缩技术 = 0.280056016805602
在学术界，以Ｈｉｎｔｏｎ、ＬｅＣｕｎ、Ｂｅｎｇｉｏ和Ｎｇ等为代表的神经网络大师们不断将深度学习的研究推向新的高峰，对包括计算机视觉、自然语言处理和机器学习在内的诸多领域带来了深远的影响［１４］ VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.17616606585441102
自２００６年深度学习出现以来，关于深度学习理论和应用方面的研究文献在国际知名期刊和会议上不断涌现，如《自然》、《科学》、ＰＡＭＩ、ＮＩＰＳ、ＣＶＰＲ、ＩＣＭＬ等 VS 3 Hierarchical hybrid memory structure图3 分层混合内存结构另外,虽然诸如PCM 等新型存储介质和DRAM 构成的混合内存在硬件技术方向的研究已经进展不小,但是其相应的软件平台研究仍相对滞后 = 0.08304547985373997
同时，由Ｂｅｎｇｉｏ等人编写的第一本关于深度学习的专著“Ｄｅｅｐ　Ｌｅａｒｎｉｎｇ”也即将由ＭＩＴ出版社出版 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.08770580193070293
包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 VS 数据种类繁多,包括传统的结构化数据,又包括文字、图片、音频和视频等非结构化数据,且非结构化数据比重在快速增长 = 0.14824986333222023
在国内，深度学习也受到了学术界的广泛关注，但目前主要是以深度学习的应用研究为主，在理论方面的工作相对较少 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.15811388300841897
以北京大学、浙江大学、上海交通大学、哈尔滨工业大学和西安电子科技大学等为代表的研究人员将深度学习算法应用到遥感图像分类［１５］、多媒体检索［１６］、交通流预测［１７］和盲图像质量评价［１８］等领域，取得了较传统方法更优的效果 VS 而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 = 0.10127393670836665
本文将以神经网络的理论和应用为主线，回顾神经网络在过去七十多年的发展历程及主要成就，重点对新近发展起来的深度学习进行阐述和讨论，并对未来的研究方向做出展望 VS 本文主要对内存计算的技术特点、分类、研究现状、热点问题和典型应用进行介绍分析,展望内容计算的发展前景 = 0.14269544824634825
Avg similar = 0.16082512007505825
