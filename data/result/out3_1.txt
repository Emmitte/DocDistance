随着大数据时代的来临,数据规模越来越大,从万亿字节(TB)到千万亿字节(PB)级 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2970442628930023
数据种类繁多,包括传统的结构化数据,又包括文字、图片、音频和视频等非结构化数据,且非结构化数据比重在快速增长 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.3086066999241838
而数据快速增长,引发的数据处理时效性难以保障 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1889822365046136
大数据所带来的大规模及需要实时处理等特点与传统的以计算为中心的模式产生巨大矛盾,使得传统计算模型难以适应当今大数据环境下的数据处理 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2553769592276246
总的来说,数据处理从以计算为中心转变成了以数据为中心,这样,通过使用传统的内存—磁盘访问模式来处理大数据总会存在I/O 瓶颈,处理的速度问题愈发突出,且时效性难以保证 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.25
现有的方案都只能一定程度上缓解这个瓶颈,而不能彻底解决这个问题 VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.15811388300841897
而大数据所表现出的增量速度快、时间局部性低等特点,客观上使得以计算为中心的传统模式面临着内存容量有限、输入/输出(I/O)压力大、缓存命中率低、数据处理的总体性能低等诸多挑战 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.10825317547305484
对此,在体系结构方面,工业界和学术界通过使用众核处理器及分布式集群,增加协处理器和GPU,使用大内存,增加I/O 通道等方式来应对 VS 传统的反向传播算法（Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ）随着传递层数的增加，残差会越来越小，出现所谓的“梯度扩散”（Ｇｒａｄｉｅｎｔ　Ｄｉｆｆｕｓｉｏｎ）现象，故而不适于深层网络的训练 = 0.08908708063747481
然而,大内存、众核处理器等能耗超高 = 0.0
文献[1,2]指出,如今大部分的DRAM 内存能耗多达40%,这对于如今的大数据处理是一个非常重要的考虑因素 VS 自２００６年深度学习出现以来，关于深度学习理论和应用方面的研究文献在国际知名期刊和会议上不断涌现，如《自然》、《科学》、ＰＡＭＩ、ＮＩＰＳ、ＣＶＰＲ、ＩＣＭＬ等 = 0.06454972243679027
在编程模型方面,研究者提出了以MapReduce[3],Hadoop[4]为代表的编程框架来解决大数据问题 VS ２００６年，Ｈｉｎｔｏｎ在《科学》上提出了一种面向复杂通用学习任务的深层神经网络，指出具有大量隐层的网络具有优异的特征学习能力，而网络的训练可以采用“逐层初始化”与“反向微调”技术解决 = 0.1355261854357877
MapReduce 在分布式系统上具有很好的可扩展性和容错性,但是它需要从磁盘获取数据,再将中间结果数据写回磁盘,导致系统的I/O 开销极大,不适用于具有实时性需求的应用 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1796053020267749
为了解决I/O 开销大的问题,近些年又衍生出许多In-Memory MapReduce 系统[5?14],即把Map 结果不再写入磁盘,而是将其写入内存,这些就避免了过多的I/O 操作,减少了开销 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.047565149415449405
然而,虽然分布式的编程框架在一定程度上解决了大数据处理的问题,但是分布式系统带来的一致性问题、节点间通信及容错数据复制等,在一定程度上限制了大数据处理的并行性 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.09805806756909202
近年来,随着多核CPU 的快速发展,内存价格的不断下降,以及系统架构的不断演进下,为大数据处理在硬件方面提供了有利的条件 VS 包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 = 0.08882311833686551
SAP 公司在2012 年推出的HANA内存计算[15]及加州大学伯克利分校开发的ApacheSpark[16]使得内存计算再次得到学术界和工业界的广泛关注 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.13608276348795434
同时,IBM 的solidDB[17]、Oracle 的Exadata X3、微软的SQLServer 2012 也引入了内存计算 VS １　引　言实现人工智能是人类长期以来一直追求的梦想 = 0.11322770341445959
内存计算不是最新提出的概念,但是近年来它却成为业界和研究领域的一个热点,解决了前面提到的大数据时代数据处理速度以及时效性的问题,其原因在于,在内存计算模式下,所有的数据在初始化阶段全部加载到内存中,数据及查询的操作都在高速内存中执行,CPU 直接从内存读取数据,进行实时地计算和分析,减少了磁盘数据访问,降低了网络与磁盘I/O 的影响,大幅提升了计算处理的数据吞吐量与处理的速度,减少了原本占大量计算资源的I/O 开销 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.3454246398538788
通过内存计算的应用,避免了I/O 瓶颈,以前在数小时、数天时间内计算的结果,在内存计算环境中,可以在数秒内完成 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.19245008972987526
因此,在高性能的计算背景下,内存计算能再次成为工业界和学界研究关注的热点,成为海量数据分析的利器则不足为奇 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2475368857441686
另外,关于大内存引起的能耗问题,近些年出现了大量新型非易失性随机存储介质,比如电阻存器、铁电存储器、相变存储器等,其容量大、价格低、读写速度与DRAM 相当,最重要的是,其能耗远远低于DRAM,因此,在一定程度可以替换DRAM 成为新型内存 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.03571428571428571
为此,出现了大量DRAM 与非易失性的混合内存的研究 VS 继美国及欧盟各国之后，我国经过两三年筹备的“中国脑科学计划”在２０１５年浮出水面，科技部正在规划“脑科学与类脑研究”的重大专项，北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究，大规模“类脑智能”的研究正蓄势待发 = 0.13363062095621217
本文主要对内存计算的技术特点、分类、研究现状、热点问题和典型应用进行介绍分析,展望内容计算的发展前景 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.242535625036333
首先,介绍和分析内存计算的概念和技术特点 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.19802950859533489
其次,给出内存计算技术及系统的分类,介绍3 种类别的原理、现状和问题 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.18490006540840973
再次,介绍内存计算的几种典型应用 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.14907119849998599
最后,从总体层面和应用层面对内存计算面临的挑战予以分析,并且对其发展前景进行展望 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.09622504486493763
1 内存计算概念内存计算不是一个新的概念,早在20 世纪90 年代就有关于内存计算雏形的论述[18,19],只是当时硬件发展有限,没有得到进一步地研究 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.2191986497404764
关于内存计算的概念,至今没有统一的定义 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.14907119849998599
Gartner 对其定义为:一种应用平台中间件,实现分布式、可靠及可扩展性、强一致或最终一致性的内存NoSQL 数据存储,可供多个应用共享[20] VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.11470786693528087
IBM给出的解释是:内存计算主要是将数据存放在服务器的内存中,以此作为数据处理加速的一个手段,其主要适用于数据访问密集型的应用[21] VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.24494897427831783
GridGrain 关于内存计算给出这样的解释:通过使用一种中间件的软件将数据存储于分布式集群中的内存当中,并且进行并行处理[22] VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.1924500897298753
TIBCO 认为,内存计算是对处理大数据所遇到瓶颈的一种突破[23] VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.16666666666666666
Techopedia 认为,随着内存价格大幅下迭,内存容量增长,这样就更好地将信息存入专用服务器内存,而不是存储速度更慢的磁盘 VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.11180339887498948
它能帮助商务用户快速地进行模式识别,及时分析大数据,即,所谓的内存计算[24] VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.19364916731037085
内存计算不仅仅是把数据驻留内存,还需要对软件体系、计算模型等进行专门的设计[25] VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.19802950859533489
因此可以看出,内存计算主要有以下特性:(1) 硬件方面拥有大容量内存,可将待处理数据尽量全部存放于内存当中,内存可以是单机内存或者分布式内存,且单机内存要足够大 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.09095085938862486
(2) 具有良好的编程模型和编程接口 VS ２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 = 0.12500000000000003
罗乐 等:内存计算技术研究综述 2149(3) 主要面向数据密集型应用,数据规模大,处理实时性要求高 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2475368857441686
(4) 大多支持并行处理数据 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.2886751345948129
综上所述,内存计算是以大数据为中心、依托计算机硬件的发展、依靠新型的软件体系结构,即,通过对体系结构及编程模型等进行重大革新,将数据装入内存中处理,而尽量避免I/O 操作的一种新型的以数据为中心的并行计算模式 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.25503068522533534
在应用层面,内存计算主要用于数据密集型计算的处理,尤其是数据量极大且需要实时分析处理的计算 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2405626121623441
这类应用以数据为中心,需要极高的数据传输及处理速率 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.17677669529663687
因此在内存计算模式中,数据的存储与传输取代了计算任务成为新的核心 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2727723627949905
内存计算与传统的内存缓存有着较大的区别,主要体现在数据在内存中的存储和访问方式上 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.18257418583505536
在内存计算中,数据长久地存储于内存中,由应用程序直接访问 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2727723627949905
即使当数据量过大导致其不能完全存放于内存中时,从应用程序视角看,待处理数据仍是存储于内存当中的,用户程序同样只是直接操作内存,而由操作系统、运行时环境完成数据在内存和磁盘间的交换 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.25802342327107
而内存缓存,利用部分内存缓存磁盘/文件数据,应用程序通过文件系统接口访问缓存中的数据,而不是像内存计算那样直接访问 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.20134681656420733
因此,内存计算和传统的内存缓存都可以通过减少I/O操作提升系统性能,内存计算支持数据直接访问,效率更高,也更适合大数据应用,缺点是不像传统内存缓存那样对应用程序透明,通常需要专门的编程模型和接口支持 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.1651445647689541
2 内存计算分类内存计算系统结构和实现方法在很大程度上取决于底层硬件架构,更准确地说,取决于底层内存架构 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1126872339638022
根据内存计算所依托硬件架构的不同,可将内存计算分为3 类:(1) 基于单节点的内存计算 VS 类脑智能是涉及计算科学、认知科学、神经科学与脑科学的交叉前沿方向 = 0.1690308509457033
(2) 基于分布集群的 = 0.0
(3) 基于新型混合结构内存的内存计算 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1259881576697424
2 = 0.0
1 基于单节点的内存计算单节点内存计算系统运行于单个物理节点上,节点拥有一个或多个处理器以及共享内存,内存结构可以是集中式共享内存,或者非一致性共享内存(non-uniform memory access,简称NUMA) VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.07362101738323103
单节点上的内存计算利用多核CPU,采用大内存和多线程并行,以充分发挥单机的计算效能,并且采取充分利用内存和CPU 的cache、优化磁盘读取等措施 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1178511301977579
在软件层面,单节点内存计算主要分为内存数据处理系统和内存存储系统两类:? 在内存数据处理系统方面,主要利用如今发展起来的众核CPU 和大内存,使得单节点系统有一定的大数据处理能力 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.12136700910941024
且其易于编程,CPU 和内存资源能被充分利用,因此具有良好的使用价值 VS ２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 = 0.03857583749052298
文献[27]中指出,拥有100G 到1TB 内存的高端服务器足够处理现实中的图数据 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.19802950859533489
近几年出现了众多基于众核内存计算的数据处理框架,比如Grace[26],Ligra[27],GRACE[28]和GraphLab[29] VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.07856742013183861
其中,Grace,Ligra 和GRACE 都利用多核CPU 和大内存,并采用了多线程并行技术,充分利用内存和CPU,只是三者处理机制及侧重点不同 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.09901475429766744
Grace 提出一种图更新聚集策略,针对图划分间(线程之间)进行了通信及负载均衡优化,提高图处理的效率 VS 神经元之间通过突触连接以相互传递信息，连接的方式和强度随着学习发生改变，从而将学习到的知识进行存储 = 0.043852900965351466
Ligra 以图为中心的计算方式,提出一种轻量级图处理框架,其重点在于使图遍历算法更容易实现 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.16666666666666666
GRACE 则重点针对同步模式,提出一种可以根据应用处理需求,由用户决定切换同步执行还是异步执行的并行图处理框架 VS 人类借助神经网络找到了处理“抽象概念”的方法，神经网络的研究又进入了一个崭新的时代［１０－１２］，深度学习的概念开始被提出 = 0.04999999999999999
而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.2595542738092201
在内存存储系统方面,单节点内存计算在数据库方面应用较多 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1111111111111111
比如早在上世纪80 年代,全内存数据库MMDB[30,31],其思路是,将整个数据库存入内存,即可加快数据处理 = 0.0
然而在当时,由于内存价值高,这种思路只是理论论述,直到近些年内存数据库对于企业级用户来说才可实现 VS 在国内，深度学习也受到了学术界的广泛关注，但目前主要是以深度学习的应用研究为主，在理论方面的工作相对较少 = 0.06933752452815364
比如:Hyper[32]是一种混合式OLTP 和OLAP 高性能内存数据库 VS ２００６年，Ｈｉｎｔｏｎ在《科学》上提出了一种面向复杂通用学习任务的深层神经网络，指出具有大量隐层的网络具有优异的特征学习能力，而网络的训练可以采用“逐层初始化”与“反向微调”技术解决 = 0.0563436169819011
Hekaton[33]是一个针对事务处理(TP)的基于行的内存数据管理系统,其为遗留应用程序提升10 倍的TP 速度、为新优化的应用提升50 倍的速度,且完全集成进SQLServer VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.09656090991705353
又如,图数据库WhiteDB[34]是建立在共享内存上的一个轻量级的NoSQL 内存数据库,它没有服务器进程,可直接对其共享内存进行读写 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.038778336716474064
Neo4j[35]同样是建立在单节点众核上的一种广泛使用的内存图数据库 VS ２００６年，Ｈｉｎｔｏｎ在《科学》上提出了一种面向复杂通用学习任务的深层神经网络，指出具有大量隐层的网络具有优异的特征学习能力，而网络的训练可以采用“逐层初始化”与“反向微调”技术解决 = 0.050964719143762556
相对分布式内存计算而言,单节点内存计算资源利用率高、处理效率高,不需要管理集群及考虑容错,也不存在节点间通信的巨大开销,系统性能也具有较强的可预估性 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.1414213562373095
从编程者的角度来看,调试及优化算法比分布式更容易 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.13608276348795434
缺点是单节点CPU、内存等资源有限,在单节点计算机上处理现实世界的大数据,则很可能面临内存不足的情况(out-of-core) VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.1404878717372541
在此情况下,内存数据处理的解决方案有3 种趋势 VS 例如，人脑被证明可以在没有导师监督的情况下主动地完成学习任务，仅凭借传统的浅层神经网络是无法实现这一点的 = 0.11785113019775793
(1) 内存压缩技术 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.280056016805602
Ligra+[36]系统就是在Ligra 系统之上添加内存压缩技术,达到和Ligra 相当的性能 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.13453455879926252
(2) 提高I/O 访问效率 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.09128709291752768
Graphchi[37]正是针对内存不足的情况,采用内存并行滑动窗口机制的一种单节点众核并行图处理框架 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.09415023063157009
PrefEdge[38]通过预取减少了固态硬盘I/O 随机访问次数 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.08536655898367085
X-Stream[39]利用了流数据的顺序访问特性,对其与随机访问进行折中 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1336306209562122
实验表明:X-Stream 在拥有3TB 磁盘的单节点系统上,可处理含640 万条边的图数据 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.14433756729740646
(3) 使用高速I/O 设备,比如固态硬盘阵列 = 0.0
FlashGraph[40]即为基于固态硬盘阵列的异步图处理框架,它采用消息传递机制,以顶点为中心,在处理数据过程中,图顶点和算法状态存于内存,边存于外存当中,其处理性超出X-Stream 和Graphchi 几个数量级 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.08111071056538127
2基于分布式系统的内存计算单节点内存计算受硬件资源限制,在处理更大规模数据时面临硬件可扩展方面的问题 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.17025130615174974
在以MapReduce 为代表的大规模分布式数据处理技术快速发展的背景下,人们也开始在分布式系统上实现内存计算 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.12524485821702988
这种内存计算利用多台计算机构成的集群构建分布式大内存,通过统一的资源调度,使待处理数据存储于分布式内存中,实现大规模数据的快速访问和处理 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.21650635094610968
根据内存计算的主要功用,可将基于分布式系统的内存计算进一步分为3种类型 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.16666666666666666
1内存存储系统近年来,磁盘容量快速增长,在过去的25 年中增长10 000 多倍,且还有继续增长的态势,但磁盘访问性能增速远低于容量增速,同时期数据传输率仅提高了50 倍,寻道时间及转动延迟仅降低为以前的1/2 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.15811388300841897
因此,磁盘I/O成了主要的数据访问瓶颈,难以满足在线海量数据处理需求 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.21650635094610968
为此,就有了这样的技术思路:将内存作为存储设备,数据全部存入内存,而将磁盘仅作为一种备份或存档工具 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.12126781251816648
RAMCloud[41]是一种典型的分布式内存数据存储模型,这种分布内存计算是通过上百台甚至上千台服务器互联,形成分布式内存存储系统,且易于扩展 VS ２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 = 0.08751750525175062
其思路是,所有数据一直缓存于内存中 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.27386127875258304
与基于磁盘的数据处理相比,该系统的延迟小100~1000 倍,吞吐量大100~1000 倍 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.06362847629757777
考虑到内存易失性问题,RAMCloud 采用复制和备份技术,以保证它具有像磁盘存储系统一样的持续性和有效性,这也保证了内存数据和磁盘数据的同步性、一致性以及容错 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.17407765595569785
但是,这样内存系统存在一定的缺点,即:其能耗很高,是磁盘系统的50~100 倍 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.16012815380508716
因此它适合有大吞吐量需求的应用,而对一些不需频繁访问且占大量存储空间的数据的应用不太适合 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1336306209562122
另一种典型内存存储系统是分布式内存数据库,包括分布式内存关系数据库,如H-store[42],VoltDB[43],ScyPer[44]等,分布式NoSQL 数据库,如MemepiC[45],Redis cluster[46]等以及分布式内存图数据库ArangoDB[47],Graph Engine[48],Sqrrl Enterprise[49],Titan[50]等 VS ２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 = 0.0468292905790847
大规模数据处理应用常常要处理PB 级的数据[51],将这些数据全部存储于内存中一方面会占用大量内存资源,增加成本和功耗 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.222717701593687
另一方面,过大的内存空间也使得数据检索和访问效率降低 = 0.0
鉴于此,在提供分布式大内存的同时,也出现了另外一项重要的技术——内存压缩[52?54] VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.13453455879926252
内存压缩技术根据压缩率的不同分为轻量级压缩[55?57]和重量级压缩[58],根据基于软硬件不同分为软件压缩[42,59]、硬件压缩[60,61]和软硬协作压缩[62,63],但其最终目的有3 个:(1) 提高访问效率,有利于查询性能的提高 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.059260885094595976
(2) 减少内存数据量,从而降低内存消耗 = 0.0
(3) 提高内存使用效率,减少CPU 等待时间,增加片间互连网络带宽利用率 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.11785113019775793
内存缓存系统利用内存作为缓存这种计算方式[64?67]已经提出20 多年,但直到近些年,随着硬件技术的发展,内存缓存系统才出现大量新的研究,比如Memcached[68],BigTable[69],PACMan[70]和GridGrain[71]等等 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.12543630150106364
内存缓存产生的技术背景是:磁盘容量往往比内存大两个数量级,这就说明,想通过内存缓存加速数据批处理,将内存作为数据永久罗乐 等:内存计算技术研究综述 2151存放处不太现实,在很多情况下,只能将内存当作缓存 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1473139127471974
因此,这类内存计算把数据分为访问频率高和低两类,将访问频率高的部分数据长久存放于内存当中,或者将一些重要数据长期缓存于内存当中 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.23717082451262844
比如,谷歌和雅虎将其检索索引全部存于内存当中,Memcached 都是将其所有通用“键-值”对存于内存中,BigTable 存储系统把它所有列族缓存于内存 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.05357997197768199
这样可以高效利用内存计算 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.16666666666666666
然而,内存局部性在很大程度上影响着作业的完成时间 = 0.0
只有当作业的所有任务都从内存中读取数据时,作业的完成才会被加速 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.21650635094610968
相反,即使只有小部分任务不是从内存读取,那么这小部分任务(outliers)会拖慢整个作业的完成时间[72] = 0.0
研究表明,大数据应用具有不同于传统应用的数据局部性特征 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.3333333333333333
一个突出表现是,大量的数据仅被访问一次 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.2886751345948129
文献[73]根据Facebook 的hadoop 日志指出:保守地看,64%的作业具有任务局部性,有75%的数据块只被访问一次,这将大大降低内存缓存的效率 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.11470786693528087
因此,对于内存缓存,仍然有很大的提升空间和面临的挑战 = 0.0
体现在如下两个方面 = 0.0
(1) 内存替换策略很多替换策略只是为了单纯的提高命中率,但是提高命中率并不能一定加快任务完成时间 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.09128709291752768
所以对于内存缓存来说,缩短任务完成时间才是最主要的目标[73] VS 如何借助神经科学、脑科学与认知科学的研究成果，研究大脑信息表征、转换机理和学习规则，建立模拟大脑信息处理过程的智能计算模型，最终使机器掌握人类的认知规律，是“类脑智能”的研究目标 = 0.057353933467640436
在众多内存替换策略中,比如LFU,LRU 等,虽然达到了一定的效果,但仍然有提升空间 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.08536655898367085
其原因在于:替换策略主要目标在于使整个作业的所有任务缓存于内存当中,以便于并行处理,消除outliers,提升整个作业执行效率 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.051298917604257706
如,文献[70]针对这种状况提出两种替换策略,其原理是:或将作业的所有并行任务缓存于内存中,或者所有任务都不缓存于内存中,即,所谓的all-or-nothing 原则 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.1231174022502185
(2) 预取对于一些只被任务访问一次的数据块,不具有内存局部性 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.17677669529663687
如前所述,某些应用中这种数据占到75%左右 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.3061862178478973
对于这种数据,只能通过预取的方式提高内存命中率,加快处理效率 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.19364916731037085
例如,一种选择就是将最新产生的数据预取进内存 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.17677669529663687
另外,如果作业有多个任务,我们可以根据第一个任务作为线索装载进其他任务的数据块 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.17677669529663687
文献[74]提出一种内存管理框架,通过前几次从磁盘上读取数据的行为预测以后要读取的数据,进而来设计出一个预取策略 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.2
将数据缓存到内存中可以大幅提高数据处理效率,但在可靠性及其带来的问题方面存在不足 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.23570226039551587
如果集群中一个节点出现故障,那么将会产生集群内节点间的大量数据移动和复制,这将带来较大的存储和通信开销 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.11180339887498948
所以,如何避免或减少容错带来的节点间数据移动和复制开销,是这类内存计算面临的一个挑战 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.16984155512168939
内存数据处理系统与前两类系统不同,此类系统从支持大数据应用角度出发,主要面向迭代式数据处理、实时数据查询等应用,通过提供编程模型/接口以及运行环境,支持这些应用在内存中进行大规模数据的分析处理和检索查询 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.21879748724684184
其处理机制是:首先将待处理数据从磁盘读入内存,此后,在这些数据上进行反复的迭代运算,即,除了第一次需要涉及I/O 操作,此后便一直从内存读写数据 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.2834733547569204
此类内存计算不涉及预取数据,而且在内存管理方面使用内存替换策略也非常高效,因此在很大程度上提高了处理效率 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.12247448713915891
近些年出现了众多此类内存计算的框架/系统,最具影响力的是加州大学伯克利分校开发的Spark,它适用于迭代式及交互式的数据批处理应用,其原理即:将数据第一次从磁盘读入内存,生成一种抽象的内存对象,即,弹性分布式数据集(resilient distributed datasets,简称RDD)[75],此后,用户程序只操作在内存当中的RDD,计算过程只涉及内存读写,因此大幅提升了数据处理效率 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.17609018126512477
Piccolo[76]同样面向需要迭代处理的应用,比如机器学习、图算法、科学计算等问题,它主要将分布式的共享中间状态以key-value 表格存于内存当中,以便高效解决分布式多线程之间共享变量的问题 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.18257418583505539
Pregel[77]和HaLoop[78]都是基于分布式内存计算的图数据处理框架,前者将中间结果存于内存用于迭代计算,后者提供一种迭代式的MapReduce 接口 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1259881576697424
Twister 同样提供了一种迭代式的MapReduce 模型,用户可创建MapReduce 作业让其迭代计算,其中,数据在迭代时被保存在内存当中 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.14708710135363803
另一种典型的内存批处理应用为M3R[79],它是MapReduce 的内存实现框架,适用于反复分析大量数据的应用,并且提供了向下兼容MapReduce 的接口,相对MapReduce 大幅提升处理效率 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.08980265101338746
另外,研究发现[80?83]:在数据密集型环境下,存在大量程序以相同或者略微不同的输入反复运行多次 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.15075567228888181
文献[84]在基于M3R系统之上提出了MapReuse 系统,使得计算重使用发生在内存数据结构中,而不是文件系统,这样极大地加快了In-Memory MapReduce 的处理速度 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.15450786078738143
另外,内存实时数据处理近年来也发展迅猛,包括Spark streaming[85],Storm[86],Yahoo!S4[87],MapReduceOnline[88]等等 VS 包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 = 0.08006407690254357
此类内存计算研究包括以下几个关键点 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.2357022603955158
(1) 容错机制内存数据的易失性,使得内存计算环境下数据恢复和容错至关重要 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.22821773229381923
MapReduce 的容错机制主要通过定期检查节点,对于出现故障的work 节点,其上的任务要重新执行 = 0.0
对于出现故障的master 节点,通过从集群其他master 上复制数据并传输到本地的方法来解决 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1336306209562122
因此,对于MapReduce 来说,容错涉及到从磁盘到内存,从集群中的其他节点到本地的数据移动 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.19364916731037085
这对于对实时性要求很高的内存计算来说是非常耗时的,因为网络数据移动的延迟远远大于本地的数据移动,吞吐量也远远小于本地数据移动 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.3572172541558802
目前的一些集群内存存储系统,比如分布式共享内存[89]、键/值存储[41]、内存数据库等,都提供了基于细粒度更新可变状态的接口,这些接口的容错方式就是通过集群中节点间的数据移动和复制,或日志更新集群间各个节点 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.08411582311380665
而这两种方法对于数据密集型负载来说开销太大,原因在于:这些方法需要在集群网点节点间复制大量数据,而网络带宽又远远小于内存带宽,这将导致大量存储开销 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.17407765595569785
Spark 采用了基于粗粒度的转化的接口,这种转化操作在计算过程中形成一种有向无环图(DAG),称为“血统(lineage)”,“血统”实质上是建立一种数据间转换的关系,而不是数据本身,因此当出现系统故障时,便可通过这个“血统”提供的信息,计算丢失的数据,即以计算换取数据移动和复制,这样即可大量节省容错所带来的开销 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.26516504294495535
同样,在Nectar[82]系统中,同样采用了“血统”容错,只是在特定编程框架下才采用“血统”容错,在传统的情形下,还是通过复制数据来解决容错问题 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.11239029738980329
Tachyon[90]也采用了“血统”,使得在运用内存缓存容错时,数据恢复得到Memory 访问速度级别的加速 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.25
另外,虽然“血统”通过计算解决了不用复制和移动数据的问题,但是当“血统”的有向无环图很大时,则需要较长的时间来计算恢复数据 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.26650089544451305
在这种情况下,Spark 和Tachyon 都采用了检查点机制来解决“血统”链很长所带的计算开销问题 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.09245003270420486
目前,容错处理主要有数据移动和复制、日志机制、分布式锁、快照、检查点机制等等 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1386750490563073
但是正如文献[80]所述,由于带宽限制,数据复制等数据恢复机制带来巨大开销,而基于“血统”容错的方法能很好地和内存计算在处理速度上相协调,因此,这可能将成为内存计算未来主要的容错方法 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.20833333333333334
(2) 同步分布式内存处理系统主要处理机器学习、图算法、科学计算等问题,此类问题在并行计算的同时,往往涉及到结构或逻辑上的依赖关系,而并行处理的各个步骤在到达稳定点的时间不同,因此需要在并行进行的计算步之间进行同步控制,以保证结果的正确性 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.20823168251814142
常见的同步方式有同步计算、异步计算和混合方式 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.2666666666666666
目前,大多数内存计算系统采用BSP[91]同步,部分系统采用异步机制 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.1432229748078866
Spark 和Pregel 都采用BSP 同步机制,而基于内存计算的分布式内存共享图处理系统PowerGraph[92]则采用BSP 同步和异步两种方式 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.09589266029707683
Trinity[93]同样采用BSP 同步和异步两种方式 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.09128709291752767
同步方式可确保计算的确定性,易于设计、编程和测试 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.17213259316477406
但是由于每个计算步的处理时间不同,最慢的计算将严重影响整体收敛速度 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.20100756305184242
异步处理模式的优势在于可加速计算的收敛速度,但其编程复杂,不便于调试和测试,不能保证更新一致性,且结果不确定 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.0890870806374748
两者各有优缺点,如何根据不同应用系统类型,取两者优点设计出高效且易于编程的同步机制越来越重要 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.07692307692307693
(3) 内存分配与管理内存计算的核心资源为内存,因而内存分配与管理显得尤为重要 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.061898446059017294
如何在内存中安排数据存储方式,使数据访问更为高效且内存资源充分利用,这是首要解决的问题 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2341464528954235
在实际的应用实例中,应用类型不同,内存数据安排也不尽相同,如Spark 系统采用“键-值”存储方式,Piccolo 采用内存表格方式及“键-值”两种方式,Pregel 采用了BigTable 的内存组织方式 VS 神经元之间通过突触连接以相互传递信息，连接的方式和强度随着学习发生改变，从而将学习到的知识进行存储 = 0.15214515486254612
另外,内存容量总是有限的,如何高效利用有限的内存资源处理海量文件又是一个挑战 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.06933752452815364
Spark 将内存数据抽象成RDD,然后在内存不足时,利用“最近最少使用”(LRU)内存替换策略协调内存资源 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1563858105428061
同时,为了更灵活地分罗乐 等:内存计算技术研究综述 2153配内存资源,Spark 可以通过所谓的RDD“持续存储优先权”给用户一定的管理权限 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.07273929674533079
总之,在此类内存计算当中,同样存在内存替换策略的选择 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1111111111111111
因此,如何根据具体的编程框架的优缺点选择高效的内存替换策略,有待进一步研究 VS 继美国及欧盟各国之后，我国经过两三年筹备的“中国脑科学计划”在２０１５年浮出水面，科技部正在规划“脑科学与类脑研究”的重大专项，北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究，大规模“类脑智能”的研究正蓄势待发 = 0.11396057645963795
(4) 网络瓶颈谷歌的一份报告[94]显示:从内存读取数据比从本地磁盘或集群网络中其他主机读取数据快两个数量级,而磁盘和集群网络读取速度又处于同一数量级 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.29488391230979427
因此,在本地处理数据时,数据存在于内存当中,CPU 很少停下来等待磁盘I/O,这样,内存计算解决了磁盘I/O 瓶颈 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.236227795630767
但是,如果涉及到节点间通信或数据传输,则要比直接在内存读写慢两个数量级,内存计算必然面临一定的效率损失 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.07856742013183861
因此,在分布式集群内存计算当中,网络瓶颈成为主要瓶颈 VS 从２０世纪４０年代的Ｍ－Ｐ神经元和Ｈｅｂｂ学习规则，到５０年代的Ｈｏｄｙｋｉｎ－Ｈｕｘｌｅｙ方程、感知器模型与自适应滤波器，再到６０年代的自组织映射网络、神经认知机、自适应共振网络，许多神经计算模型都发展成为信号处理、计算机视觉、自然语言处理与优化计算等领域的经典方法，为该领域带来了里程碑式的影响 = 0.14285714285714285
如何减少节点间的通信和数据传输开销,提高内存计算效率,这又是一个挑战 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.11785113019775793
3 新型混合内存结构的内存计算近几年, 新兴的非易失性随机存储介质(non-volatile memory, 简称NVM) 快速发展, 如铁电存储器(ferroelectric random accessmemory,简称FeRAM)[95]、相变存储器(phase change memory,简称PCM)[96?98]、电阻存储器(resistive randomaccess memory,简称RRAM)[99]等,其性能接近DRAM,但容量远远大于DRAM,同时,能耗和价格远远低于DRAM VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.062217101683825514
这为新型的内存体系结构提供了良好的硬件保障 VS 包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 = 0.06537204504606135
因此,基于新型存储器件和传统DRAM 的新型混合内存体系在大幅提升内存容量,降低成本的同时,其访问速度与DRAM 相当 VS 例如，人脑被证明可以在没有导师监督的情况下主动地完成学习任务，仅凭借传统的浅层神经网络是无法实现这一点的 = 0.06454972243679027
在众多的非易失性随机存储介质中,PCM 凭借其非易失性、非破坏性读、读完无须回写、写操作无须先擦除、存储密度高等特性,逐渐成为大规模内存系统中颇具潜力的DRAM 替代品[98] VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.1164445019479164
在硬件体系结构方面,研究者围绕PCM 和DRAM 的混合方案开展了很多研究,国内外学术界出现的混合内存结构大概分为3类 VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.09428090415820635
1 线性统一编址混合内存这种混合内存结构由PCM 和DRAM 构成[100,101] VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.1
通过软件和硬件技术,避免了PCM 较短的写寿命和较高的写能耗的缺点,充分发挥了PCM 在读数据和存储数据方面低功耗、非易失性和DRAM 在写数据时低功耗及超长的写寿命的特性 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.23385358667337133
实验表明:相比DRAM,PDRAM 的能耗节省达到37% VS 包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 = 0.06052275326688023
在PDRAM 中,PCM 和DRAM 处于同等地位,无主次之分,对两者进行统一的线性编址,如图1 所示 VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.05590169943749474
Fig = 0.0
1 Hybrid memory structure sharing a single physical address space图1 线性统一编址混合内存结构 VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.09701425001453319
2 以DRAM 作为PCM 缓存的混合内存这种混合型内存体系结构是由DRAM 和PCM 共同组成,其中,PCM 作为主存,DRAM 作为PCM 的缓存,其主要目的是利用PCM 高容量以及DRAM 快速访问的特点,同时,避免了PCM 访问速度慢及DRAM 容量小 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.02711630722733202
另外,文献[104]中通过3 种技术来减少PCM 上的写操作,以达到延长PCM 寿命的目的 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.12964074471043288
实验结果表明,这种内存系统较传统的内存容量提高了4 倍,页错误平均降低5 倍,PCM写操作次数降低了3 倍,可延长PCM 寿命3~9 VS 例如，人脑被证明可以在没有导师监督的情况下主动地完成学习任务，仅凭借传统的浅层神经网络是无法实现这一点的 = 0.05184758473652128
7 年 VS ２０１３年１月，作为百度公司创始人兼ＣＥＯ的李彦宏在其年会上宣布了成立百度研究院的计划，并且强调首当其冲的就是组建“深度学习研究所” = 0.20412414523193154
且当DRAM 容量为PCM 的3%时,PCM 较DRAM 的访问延迟得到了很多的弥补 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1336306209562122
图2 描述了这种内存系统与传统内存系统的区别 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 0.16012815380508716
L1 cacheL2 cacheCPUMain memoryDRAM cachePCM memoryFig = 0.0
2 Hybrid memory structure using DRAM as cache图2 DRAM 缓存混合内存结构 VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.10327955589886445
3 分层混合内存(hierarchical hybrid memory)文献[105]中提出一种名叫MN-MATE 的混合内存 VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.0512989176042577
这种内存是由PCM 和DRAM 构成,具有层次结构 VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.17888543819998318
这种层次内存分为片上和片下两部分:片上内存由单独的DRAM 构成,因其内置于处理器因此具有较小的延迟 VS 深层神经网络通过增加网络的层数来模拟人脑复杂的层次化认知规律，以使机器获得“抽象概念”的能力，在无监督特征学习方面具有更强的能力 = 0.04347826086956522
片下部分则由PCM和DRAM 混合构成,两者共用同一个内存控制器且采用统一的线性编址 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.06063390625908324
如图3 所示,M1 为上片,M2 为下片,分别由PCM 和DRAM 通过线性编址组成 VS 神经网络的计算结构和学习规则遵照生物神经网络设计，在数字计算机中，神经细胞接收周围细胞的刺激并产生相应输出信号的过程可以用“线性加权和”及“函数映射”的方式来模拟，而网络结构和权值调整的过程用优化学习算法实现 = 0.04441155916843276
同样,这种内存结构通过3 种技术发挥了PCM 的大容量低能耗以及DRAM 访问速度快的优点,提高内存性能和减少能耗 VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 0.11128297681493142
CacheM1 memoryCPULinera address spaceM2 memoryPCM memory DRAM cacheFig = 0.0
3 Hierarchical hybrid memory structure图3 分层混合内存结构另外,虽然诸如PCM 等新型存储介质和DRAM 构成的混合内存在硬件技术方向的研究已经进展不小,但是其相应的软件平台研究仍相对滞后 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.1856953381770519
现阶段想要利用新型混合内存处理实时大数据,仍然有很多工作做要 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.15811388300841897
罗乐 等:内存计算技术研究综述 2155虽然各种新型NVM 在容量、能耗方面比当前的DRAM 更具有优势,但是在访问速度、写寿命等方面仍然不及DRAM[96] VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.1
比如:对于PCM 存在的缺点,文献[106]通过使用消除冗余位写入、行替换和段交换等方法降低损耗,可延长PCM 的使用寿命13 到22 年 VS 除了以上优势外，深度学习最具吸引力的地方还在于能凭借无标签的数据来进行学习，而不需要依赖于监督信息的支撑［１３］ = 0.049147318718299055
为了解决混合内存中PCM 写数据速度慢且写寿命较短的问题,文献[107,108]分别提出不同的内存管理技术,通过不同的预测写操作算法,让大多数写操作发生在DRAM 而非PCM,从而延长了PCM 的写寿命,并且可以隐藏PCM 写数据慢的缺点 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.10818558060197887
文献[109]不仅采用减少PCM 写次数的方法延长PCM 寿命,还采用了提高DRAM 缓存命中率的方法,从两个方面来优化混合内存结构的能耗和性能 VS 人类借助神经网络找到了处理“抽象概念”的方法，神经网络的研究又进入了一个崭新的时代［１０－１２］，深度学习的概念开始被提出 = 0.08304547985373997
此外,针对PCM 等新型随机存储介质的缺点,还有众多研究从软硬件层面提出了很多的改进措施,也取得了不错的效果[98,110,111] VS 继美国及欧盟各国之后，我国经过两三年筹备的“中国脑科学计划”在２０１５年浮出水面，科技部正在规划“脑科学与类脑研究”的重大专项，北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究，大规模“类脑智能”的研究正蓄势待发 = 0.0890870806374748
在硬件体系结构方面,这种内存计算的主要革新表现在:在原来的DRAM 基础上,通过加入新型SCM 扩展成了不同类型的混合结构大内存,访问速度接近于DRAM,而容量远远大于DRAM VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.08980265101338746
在这种内存计算模式中,SCM 不仅可作为内存,而且相当于传统磁盘,计算可以直接发生在SCM 上,避免了传统的内存—磁盘数据访问,CPU 直接从内存读取数据,进行计算分析 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.2286647801900118
同时,这种新型内存架构也带来了一系列问题:(1) 各种新型内存与DRAM 存在访存速度、能耗、读写寿命等方面差异,如何协调这些差异,使整个内存处理效率较高,能耗较少?(2) 随着众核处理器性能越来越高,异构内存容量增大,如何处理大内存与众核处理器间不断加大的带宽鸿沟?(3) 内存较原来更大,如何应对大内存带来的高能耗问题? VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.05360562674188974
相对于硬件体系结构,在软件方面,这种内存计算的相关研究滞后很多 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 0.2222222222222222
因为作为一种新兴的混合内存结构,它涉及到体系结构、操作系统、编程模型方面的诸多问题 VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 0.12060453783110546
(1) 体系结构由于新型内存替代了磁盘可以长期存储数据,那么传统的磁盘数据访问局部性将被内存访问局部性取代而成为性能优化的主要目标 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 0.18257418583505536
并且,传统的磁盘数据预取、缓存以及替换策略无法直接迁移到非一致缓存访问以及非一致内存访问环境中 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 0.12500000000000003
(2) 操作系统由于新型混合结构的出现,操作系统需要统一管理多种异构资源,实现高效的、透明的、可靠的内存访问与管理策略 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 0.02488363008967198
另外,由于新型内存容量增大,可用地址空间随之增大,致使操作系统内存管理开销增大 = 0.0
所以,为了提高操作系统对内存管理效率,需要研究新的编址策略和访问方式 VS 类脑智能的实现离不开大脑神经系统的研究 = 0.1091089451179962
(3) 编程模型新型混合内存解决了磁盘I/O 瓶颈问题,因此在内存计算当中,传统的编程模型对数据传输不再占大量处理时间,而数据处理将占据大量处理时间 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.1414213562373095
此外,由于新型非易失性内存的I/O 延迟远小于磁盘,传统模型中磁盘与内存的数据一致性等问题不复存在,编程模型也因此有相应改变 VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.12456821978060995
Avg similar = 0.1376097884442574
