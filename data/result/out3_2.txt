随着大数据时代的来临,数据规模越来越大,从万亿字节(TB)到千万亿字节(PB)级 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1143323900950059
数据种类繁多,包括传统的结构化数据,又包括文字、图片、音频和视频等非结构化数据,且非结构化数据比重在快速增长 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1454785934906616
而数据快速增长,引发的数据处理时效性难以保障 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.0890870806374748
大数据所带来的大规模及需要实时处理等特点与传统的以计算为中心的模式产生巨大矛盾,使得传统计算模型难以适应当今大数据环境下的数据处理 VS Ｍ－Ｐ模型是对 生物神经元信息处理模式的数学简化，后续的神经 网络研究工作都是以它为基础的 = 0.11145564251507058
总的来说,数据处理从以计算为中心转变成了以数据为中心,这样,通过使用传统的内存—磁盘访问模式来处理大数据总会存在I/O 瓶颈,处理的速度问题愈发突出,且时效性难以保证 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.09428090415820635
现有的方案都只能一定程度上缓解这个瓶颈,而不能彻底解决这个问题 = 0.0
而大数据所表现出的增量速度快、时间局部性低等特点,客观上使得以计算为中心的传统模式面临着内存容量有限、输入/输出(I/O)压力大、缓存命中率低、数据处理的总体性能低等诸多挑战 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.125
对此,在体系结构方面,工业界和学术界通过使用众核处理器及分布式集群,增加协处理器和GPU,使用大内存,增加I/O 通道等方式来应对 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.08492077756084469
然而,大内存、众核处理器等能耗超高 = 0.0
文献[1,2]指出,如今大部分的DRAM 内存能耗多达40%,这对于如今的大数据处理是一个非常重要的考虑因素 VS 图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 = 0.035533452725935076
在编程模型方面,研究者提出了以MapReduce[3],Hadoop[4]为代表的编程框架来解决大数据问题 VS １９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］ = 0.10286889997472794
MapReduce 在分布式系统上具有很好的可扩展性和容错性,但是它需要从磁盘获取数据,再将中间结果数据写回磁盘,导致系统的I/O 开销极大,不适用于具有实时性需求的应用 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08466675133346035
为了解决I/O 开销大的问题,近些年又衍生出许多In-Memory MapReduce 系统[5?14],即把Map 结果不再写入磁盘,而是将其写入内存,这些就避免了过多的I/O 操作,减少了开销 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.14269544824634822
然而,虽然分布式的编程框架在一定程度上解决了大数据处理的问题,但是分布式系统带来的一致性问题、节点间通信及容错数据复制等,在一定程度上限制了大数据处理的并行性 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.04622501635210243
近年来,随着多核CPU 的快速发展,内存价格的不断下降,以及系统架构的不断演进下,为大数据处理在硬件方面提供了有利的条件 = 0.0
SAP 公司在2012 年推出的HANA内存计算[15]及加州大学伯克利分校开发的ApacheSpark[16]使得内存计算再次得到学术界和工业界的广泛关注 = 0.0
同时,IBM 的solidDB[17]、Oracle 的Exadata X3、微软的SQLServer 2012 也引入了内存计算 VS １９５８年，Ｒｏｓｅｎｂｌａｔｔ等人成功研制出了代号为 Ｍａｒｋ　Ｉ的感知机（Ｐｅｒｃｅｐｔｒｏｎ），这是历史上首个将 神经网络的学习功能用于模式识别的装置，标志着 神经网路进入了新的发展阶段［２２］ = 0.05063696835418333
内存计算不是最新提出的概念,但是近年来它却成为业界和研究领域的一个热点,解决了前面提到的大数据时代数据处理速度以及时效性的问题,其原因在于,在内存计算模式下,所有的数据在初始化阶段全部加载到内存中,数据及查询的操作都在高速内存中执行,CPU 直接从内存读取数据,进行实时地计算和分析,减少了磁盘数据访问,降低了网络与磁盘I/O 的影响,大幅提升了计算处理的数据吞吐量与处理的速度,减少了原本占大量计算资源的I/O 开销 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.15194743527951726
通过内存计算的应用,避免了I/O 瓶颈,以前在数小时、数天时间内计算的结果,在内存计算环境中,可以在数秒内完成 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.08006407690254358
因此,在高性能的计算背景下,内存计算能再次成为工业界和学界研究关注的热点,成为海量数据分析的利器则不足为奇 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.1143323900950059
另外,关于大内存引起的能耗问题,近些年出现了大量新型非易失性随机存储介质,比如电阻存器、铁电存储器、相变存储器等,其容量大、价格低、读写速度与DRAM 相当,最重要的是,其能耗远远低于DRAM,因此,在一定程度可以替换DRAM 成为新型内存 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.033671751485073696
为此,出现了大量DRAM 与非易失性的混合内存的研究 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.16666666666666666
本文主要对内存计算的技术特点、分类、研究现状、热点问题和典型应用进行介绍分析,展望内容计算的发展前景 VS 感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测 = 0.140028008402801
首先,介绍和分析内存计算的概念和技术特点 VS １９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］ = 0.07856742013183861
其次,给出内存计算技术及系统的分类,介绍3 种类别的原理、现状和问题 VS 感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测 = 0.19245008972987526
再次,介绍内存计算的几种典型应用 = 0.0
最后,从总体层面和应用层面对内存计算面临的挑战予以分析,并且对其发展前景进行展望 VS １９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］ = 0.05555555555555555
1 内存计算概念内存计算不是一个新的概念,早在20 世纪90 年代就有关于内存计算雏形的论述[18,19],只是当时硬件发展有限,没有得到进一步地研究 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.07749842582921286
关于内存计算的概念,至今没有统一的定义 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.08304547985373997
Gartner 对其定义为:一种应用平台中间件,实现分布式、可靠及可扩展性、强一致或最终一致性的内存NoSQL 数据存储,可供多个应用共享[20] VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.10814761408717503
IBM给出的解释是:内存计算主要是将数据存放在服务器的内存中,以此作为数据处理加速的一个手段,其主要适用于数据访问密集型的应用[21] VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.14142135623730953
GridGrain 关于内存计算给出这样的解释:通过使用一种中间件的软件将数据存储于分布式集群中的内存当中,并且进行并行处理[22] VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1666666666666667
TIBCO 认为,内存计算是对处理大数据所遇到瓶颈的一种突破[23] VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.07856742013183861
Techopedia 认为,随着内存价格大幅下迭,内存容量增长,这样就更好地将信息存入专用服务器内存,而不是存储速度更慢的磁盘 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.05892556509887897
它能帮助商务用户快速地进行模式识别,及时分析大数据,即,所谓的内存计算[24] VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.07453559924999299
内存计算不仅仅是把数据驻留内存,还需要对软件体系、计算模型等进行专门的设计[25] VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.13068205256070964
因此可以看出,内存计算主要有以下特性:(1) 硬件方面拥有大容量内存,可将待处理数据尽量全部存放于内存当中,内存可以是单机内存或者分布式内存,且单机内存要足够大 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.028583097523751475
(2) 具有良好的编程模型和编程接口 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.1466471150213533
罗乐 等:内存计算技术研究综述 2149(3) 主要面向数据密集型应用,数据规模大,处理实时性要求高 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1143323900950059
(4) 大多支持并行处理数据 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.13608276348795434
综上所述,内存计算是以大数据为中心、依托计算机硬件的发展、依靠新型的软件体系结构,即,通过对体系结构及编程模型等进行重大革新,将数据装入内存中处理,而尽量避免I/O 操作的一种新型的以数据为中心的并行计算模式 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.14724203476646205
在应用层面,内存计算主要用于数据密集型计算的处理,尤其是数据量极大且需要实时分析处理的计算 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.055555555555555566
这类应用以数据为中心,需要极高的数据传输及处理速率 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08333333333333333
因此在内存计算模式中,数据的存储与传输取代了计算任务成为新的核心 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.18898223650461363
内存计算与传统的内存缓存有着较大的区别,主要体现在数据在内存中的存储和访问方式上 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.15811388300841897
在内存计算中,数据长久地存储于内存中,由应用程序直接访问 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.25197631533948484
即使当数据量过大导致其不能完全存放于内存中时,从应用程序视角看,待处理数据仍是存储于内存当中的,用户程序同样只是直接操作内存,而由操作系统、运行时环境完成数据在内存和磁盘间的交换 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.17376201171422898
而内存缓存,利用部分内存缓存磁盘/文件数据,应用程序通过文件系统接口访问缓存中的数据,而不是像内存计算那样直接访问 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1162476387438193
因此,内存计算和传统的内存缓存都可以通过减少I/O操作提升系统性能,内存计算支持数据直接访问,效率更高,也更适合大数据应用,缺点是不像传统内存缓存那样对应用程序透明,通常需要专门的编程模型和接口支持 VS 图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 = 0.06639061303092922
2 内存计算分类内存计算系统结构和实现方法在很大程度上取决于底层硬件架构,更准确地说,取决于底层内存架构 VS 感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测 = 0.1301200097264711
根据内存计算所依托硬件架构的不同,可将内存计算分为3 类:(1) 基于单节点的内存计算 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.040522044923655395
(2) 基于分布集群的 = 0.0
(3) 基于新型混合结构内存的内存计算 = 0.0
2 = 0.0
1 基于单节点的内存计算单节点内存计算系统运行于单个物理节点上,节点拥有一个或多个处理器以及共享内存,内存结构可以是集中式共享内存,或者非一致性共享内存(non-uniform memory access,简称NUMA) VS 以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 = 0.024693239916239736
单节点上的内存计算利用多核CPU,采用大内存和多线程并行,以充分发挥单机的计算效能,并且采取充分利用内存和CPU 的cache、优化磁盘读取等措施 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.04498201079280504
在软件层面,单节点内存计算主要分为内存数据处理系统和内存存储系统两类:? 在内存数据处理系统方面,主要利用如今发展起来的众核CPU 和大内存,使得单节点系统有一定的大数据处理能力 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.05417289784878865
且其易于编程,CPU 和内存资源能被充分利用,因此具有良好的使用价值 = 0.0
文献[27]中指出,拥有100G 到1TB 内存的高端服务器足够处理现实中的图数据 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.17149858514250885
近几年出现了众多基于众核内存计算的数据处理框架,比如Grace[26],Ligra[27],GRACE[28]和GraphLab[29] = 0.0
其中,Grace,Ligra 和GRACE 都利用多核CPU 和大内存,并采用了多线程并行技术,充分利用内存和CPU,只是三者处理机制及侧重点不同 = 0.0
Grace 提出一种图更新聚集策略,针对图划分间(线程之间)进行了通信及负载均衡优化,提高图处理的效率 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.08158924398306318
Ligra 以图为中心的计算方式,提出一种轻量级图处理框架,其重点在于使图遍历算法更容易实现 VS 图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 = 0.06154574548966636
GRACE 则重点针对同步模式,提出一种可以根据应用处理需求,由用户决定切换同步执行还是异步执行的并行图处理框架 VS Ｍ－Ｐ模型是对 生物神经元信息处理模式的数学简化，后续的神经 网络研究工作都是以它为基础的 = 0.05976143046671968
而GraphLab 是一种基于图模型的处理机器学习算法的异步并行框架 VS 继Ｈｅｂｂ 学习规则之后，神经元的有监督Ｄｅｌｔａ学习规则被 提出，用以解决在输入输出已知的情况下神经元权 值的学习问题 = 0.17616606585441102
在内存存储系统方面,单节点内存计算在数据库方面应用较多 = 0.0
比如早在上世纪80 年代,全内存数据库MMDB[30,31],其思路是,将整个数据库存入内存,即可加快数据处理 = 0.0
然而在当时,由于内存价值高,这种思路只是理论论述,直到近些年内存数据库对于企业级用户来说才可实现 = 0.0
比如:Hyper[32]是一种混合式OLTP 和OLAP 高性能内存数据库 = 0.0
Hekaton[33]是一个针对事务处理(TP)的基于行的内存数据管理系统,其为遗留应用程序提升10 倍的TP 速度、为新优化的应用提升50 倍的速度,且完全集成进SQLServer VS 感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测 = 0.0670025210172808
又如,图数据库WhiteDB[34]是建立在共享内存上的一个轻量级的NoSQL 内存数据库,它没有服务器进程,可直接对其共享内存进行读写 VS 图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 = 0.02326210525996177
Neo4j[35]同样是建立在单节点众核上的一种广泛使用的内存图数据库 VS 图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 = 0.037113480951260276
相对分布式内存计算而言,单节点内存计算资源利用率高、处理效率高,不需要管理集群及考虑容错,也不存在节点间通信的巨大开销,系统性能也具有较强的可预估性 = 0.0
从编程者的角度来看,调试及优化算法比分布式更容易 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.10388150415966657
缺点是单节点CPU、内存等资源有限,在单节点计算机上处理现实世界的大数据,则很可能面临内存不足的情况(out-of-core) VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.05407380704358752
在此情况下,内存数据处理的解决方案有3 种趋势 VS 继Ｈｅｂｂ 学习规则之后，神经元的有监督Ｄｅｌｔａ学习规则被 提出，用以解决在输入输出已知的情况下神经元权 值的学习问题 = 0.07580980435789035
(1) 内存压缩技术 = 0.0
Ligra+[36]系统就是在Ligra 系统之上添加内存压缩技术,达到和Ligra 相当的性能 = 0.0
(2) 提高I/O 访问效率 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.1860521018838127
Graphchi[37]正是针对内存不足的情况,采用内存并行滑动窗口机制的一种单节点众核并行图处理框架 VS 图１　神经元的Ｍ－Ｐ模型示意 图中，ｘｉ（ｉ＝１，２，…，ｎ）表示来自与当前神经元 相连的其他神经元传递的输入信号，ｗｉｊ 代表从神经 元ｊ 到神经元ｉ的连接强度或权值，θｉ 为神经元的激 活阈值或偏置，ｆ 称作激活函数或转移函数 = 0.028239124736245246
PrefEdge[38]通过预取减少了固态硬盘I/O 随机访问次数 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.15384615384615385
X-Stream[39]利用了流数据的顺序访问特性,对其与随机访问进行折中 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.06800640801679171
实验表明:X-Stream 在拥有3TB 磁盘的单节点系统上,可处理含640 万条边的图数据 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.06804138174397717
(3) 使用高速I/O 设备,比如固态硬盘阵列 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.15724272550828775
FlashGraph[40]即为基于固态硬盘阵列的异步图处理框架,它采用消息传递机制,以顶点为中心,在处理数据过程中,图顶点和算法状态存于内存,边存于外存当中,其处理性超出X-Stream 和Graphchi 几个数量级 VS Ｈｅｂｂ规则隶属于无监督学习算法的范畴，其 主要思想是根据两个神经元的激发状态来调整其连 接关系，以此实现对简单神经活动的模拟 = 0.07254762501100116
2基于分布式系统的内存计算单节点内存计算受硬件资源限制,在处理更大规模数据时面临硬件可扩展方面的问题 VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.07490058862731383
在以MapReduce 为代表的大规模分布式数据处理技术快速发展的背景下,人们也开始在分布式系统上实现内存计算 = 0.0
这种内存计算利用多台计算机构成的集群构建分布式大内存,通过统一的资源调度,使待处理数据存储于分布式内存中,实现大规模数据的快速访问和处理 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.16666666666666666
根据内存计算的主要功用,可将基于分布式系统的内存计算进一步分为3种类型 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.046423834544262965
1内存存储系统近年来,磁盘容量快速增长,在过去的25 年中增长10 000 多倍,且还有继续增长的态势,但磁盘访问性能增速远低于容量增速,同时期数据传输率仅提高了50 倍,寻道时间及转动延迟仅降低为以前的1/2 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.037267799624996496
因此,磁盘I/O成了主要的数据访问瓶颈,难以满足在线海量数据处理需求 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.12009611535381537
为此,就有了这样的技术思路:将内存作为存储设备,数据全部存入内存,而将磁盘仅作为一种备份或存档工具 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1143323900950059
RAMCloud[41]是一种典型的分布式内存数据存储模型,这种分布内存计算是通过上百台甚至上千台服务器互联,形成分布式内存存储系统,且易于扩展 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08084520834544433
其思路是,所有数据一直缓存于内存中 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.21081851067789195
与基于磁盘的数据处理相比,该系统的延迟小100~1000 倍,吞吐量大100~1000 倍 = 0.0
考虑到内存易失性问题,RAMCloud 采用复制和备份技术,以保证它具有像磁盘存储系统一样的持续性和有效性,这也保证了内存数据和磁盘数据的同步性、一致性以及容错 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08206099398622183
但是,这样内存系统存在一定的缺点,即:其能耗很高,是磁盘系统的50~100 倍 = 0.0
因此它适合有大吞吐量需求的应用,而对一些不需频繁访问且占大量存储空间的数据的应用不太适合 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.06299407883487121
另一种典型内存存储系统是分布式内存数据库,包括分布式内存关系数据库,如H-store[42],VoltDB[43],ScyPer[44]等,分布式NoSQL 数据库,如MemepiC[45],Redis cluster[46]等以及分布式内存图数据库ArangoDB[47],Graph Engine[48],Sqrrl Enterprise[49],Titan[50]等 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.02703690352179376
大规模数据处理应用常常要处理PB 级的数据[51],将这些数据全部存储于内存中一方面会占用大量内存资源,增加成本和功耗 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.15430334996209194
另一方面,过大的内存空间也使得数据检索和访问效率降低 = 0.0
鉴于此,在提供分布式大内存的同时,也出现了另外一项重要的技术——内存压缩[52?54] VS １９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］ = 0.05337605126836238
内存压缩技术根据压缩率的不同分为轻量级压缩[55?57]和重量级压缩[58],根据基于软硬件不同分为软件压缩[42,59]、硬件压缩[60,61]和软硬协作压缩[62,63],但其最终目的有3 个:(1) 提高访问效率,有利于查询性能的提高 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.04537259256929324
(2) 减少内存数据量,从而降低内存消耗 = 0.0
(3) 提高内存使用效率,减少CPU 等待时间,增加片间互连网络带宽利用率 VS 从直观上来说，当神经元ｉ的实际输出比期望 输出大，则减小与已激活神经元的连接权重，同时增 加与已抑制神经元的连接权重；当神经元ｉ的实际 输出比期望输出小，则增加与已激活神经元的连接 权重，同时减小与已抑制神经元的连接权重 = 0.027777777777777776
内存缓存系统利用内存作为缓存这种计算方式[64?67]已经提出20 多年,但直到近些年,随着硬件技术的发展,内存缓存系统才出现大量新的研究,比如Memcached[68],BigTable[69],PACMan[70]和GridGrain[71]等等 VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.08122955416108235
内存缓存产生的技术背景是:磁盘容量往往比内存大两个数量级,这就说明,想通过内存缓存加速数据批处理,将内存作为数据永久罗乐 等:内存计算技术研究综述 2151存放处不太现实,在很多情况下,只能将内存当作缓存 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.055555555555555566
因此,这类内存计算把数据分为访问频率高和低两类,将访问频率高的部分数据长久存放于内存当中,或者将一些重要数据长期缓存于内存当中 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1118033988749895
比如,谷歌和雅虎将其检索索引全部存于内存当中,Memcached 都是将其所有通用“键-值”对存于内存中,BigTable 存储系统把它所有列族缓存于内存 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08206099398622183
这样可以高效利用内存计算 = 0.0
然而,内存局部性在很大程度上影响着作业的完成时间 = 0.0
只有当作业的所有任务都从内存中读取数据时,作业的完成才会被加速 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.13608276348795434
相反,即使只有小部分任务不是从内存读取,那么这小部分任务(outliers)会拖慢整个作业的完成时间[72] = 0.0
研究表明,大数据应用具有不同于传统应用的数据局部性特征 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.15713484026367722
一个突出表现是,大量的数据仅被访问一次 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.13608276348795434
文献[73]根据Facebook 的hadoop 日志指出:保守地看,64%的作业具有任务局部性,有75%的数据块只被访问一次,这将大大降低内存缓存的效率 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.05407380704358752
因此,对于内存缓存,仍然有很大的提升空间和面临的挑战 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.1403724812687193
体现在如下两个方面 = 0.0
(1) 内存替换策略很多替换策略只是为了单纯的提高命中率,但是提高命中率并不能一定加快任务完成时间 = 0.0
所以对于内存缓存来说,缩短任务完成时间才是最主要的目标[73] = 0.0
在众多内存替换策略中,比如LFU,LRU 等,虽然达到了一定的效果,但仍然有提升空间 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.10300524052492097
其原因在于:替换策略主要目标在于使整个作业的所有任务缓存于内存当中,以便于并行处理,消除outliers,提升整个作业执行效率 = 0.0
如,文献[70]针对这种状况提出两种替换策略,其原理是:或将作业的所有并行任务缓存于内存中,或者所有任务都不缓存于内存中,即,所谓的all-or-nothing 原则 VS Ｈｅｂｂ学习规则和Ｄｅｌｔａ学习规则都是针对单 个神经元而提出的，在神经元组成的网络中参数的 学习规则将会在后续述及 = 0.1044465935734187
(2) 预取对于一些只被任务访问一次的数据块,不具有内存局部性 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08333333333333333
如前所述,某些应用中这种数据占到75%左右 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.23570226039551587
对于这种数据,只能通过预取的方式提高内存命中率,加快处理效率 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.07453559924999299
例如,一种选择就是将最新产生的数据预取进内存 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08333333333333333
另外,如果作业有多个任务,我们可以根据第一个任务作为线索装载进其他任务的数据块 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08333333333333333
文献[74]提出一种内存管理框架,通过前几次从磁盘上读取数据的行为预测以后要读取的数据,进而来设计出一个预取策略 VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.10776318121606494
将数据缓存到内存中可以大幅提高数据处理效率,但在可靠性及其带来的问题方面存在不足 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.13608276348795434
如果集群中一个节点出现故障,那么将会产生集群内节点间的大量数据移动和复制,这将带来较大的存储和通信开销 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.10540925533894598
所以,如何避免或减少容错带来的节点间数据移动和复制开销,是这类内存计算面临的一个挑战 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.06537204504606135
内存数据处理系统与前两类系统不同,此类系统从支持大数据应用角度出发,主要面向迭代式数据处理、实时数据查询等应用,通过提供编程模型/接口以及运行环境,支持这些应用在内存中进行大规模数据的分析处理和检索查询 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1375228328345058
其处理机制是:首先将待处理数据从磁盘读入内存,此后,在这些数据上进行反复的迭代运算,即,除了第一次需要涉及I/O 操作,此后便一直从内存读写数据 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1336306209562122
此类内存计算不涉及预取数据,而且在内存管理方面使用内存替换策略也非常高效,因此在很大程度上提高了处理效率 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.047140452079103175
近些年出现了众多此类内存计算的框架/系统,最具影响力的是加州大学伯克利分校开发的Spark,它适用于迭代式及交互式的数据批处理应用,其原理即:将数据第一次从磁盘读入内存,生成一种抽象的内存对象,即,弹性分布式数据集(resilient distributed datasets,简称RDD)[75],此后,用户程序只操作在内存当中的RDD,计算过程只涉及内存读写,因此大幅提升了数据处理效率 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.10166571355506979
Piccolo[76]同样面向需要迭代处理的应用,比如机器学习、图算法、科学计算等问题,它主要将分布式的共享中间状态以key-value 表格存于内存当中,以便高效解决分布式多线程之间共享变量的问题 VS 继Ｈｅｂｂ 学习规则之后，神经元的有监督Ｄｅｌｔａ学习规则被 提出，用以解决在输入输出已知的情况下神经元权 值的学习问题 = 0.1356127007241621
Pregel[77]和HaLoop[78]都是基于分布式内存计算的图数据处理框架,前者将中间结果存于内存用于迭代计算,后者提供一种迭代式的MapReduce 接口 VS 不失一般性，首先将ｗ 和ｂ 的初始值设为０，对于 误分类点按照式（１０）和（１１）的规则来对ｗ 和ｂ 的 值进行更新 = 0.03940552031195503
Twister 同样提供了一种迭代式的MapReduce 模型,用户可创建MapReduce 作业让其迭代计算,其中,数据在迭代时被保存在内存当中 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.10567049305145206
另一种典型的内存批处理应用为M3R[79],它是MapReduce 的内存实现框架,适用于反复分析大量数据的应用,并且提供了向下兼容MapReduce 的接口,相对MapReduce 大幅提升处理效率 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.04233337566673018
另外,研究发现[80?83]:在数据密集型环境下,存在大量程序以相同或者略微不同的输入反复运行多次 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.1421338109037403
文献[84]在基于M3R系统之上提出了MapReuse 系统,使得计算重使用发生在内存数据结构中,而不是文件系统,这样极大地加快了In-Memory MapReduce 的处理速度 VS １９４９年，在《行为 的组织》一书中心理学家Ｈｅｂｂ对神经元之间连接 强度的变化规则进行了分析，并基于此提出了著名 的Ｈｅｂｂ学习规则［２０］ = 0.07147416898918632
另外,内存实时数据处理近年来也发展迅猛,包括Spark streaming[85],Storm[86],Yahoo!S4[87],MapReduceOnline[88]等等 = 0.0
此类内存计算研究包括以下几个关键点 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.16666666666666666
(1) 容错机制内存数据的易失性,使得内存计算环境下数据恢复和容错至关重要 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.10540925533894598
MapReduce 的容错机制主要通过定期检查节点,对于出现故障的work 节点,其上的任务要重新执行 = 0.0
对于出现故障的master 节点,通过从集群其他master 上复制数据并传输到本地的方法来解决 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.06299407883487121
因此,对于MapReduce 来说,容错涉及到从磁盘到内存,从集群中的其他节点到本地的数据移动 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.14907119849998599
这对于对实时性要求很高的内存计算来说是非常耗时的,因为网络数据移动的延迟远远大于本地的数据移动,吞吐量也远远小于本地数据移动 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.14433756729740646
目前的一些集群内存存储系统,比如分布式共享内存[89]、键/值存储[41]、内存数据库等,都提供了基于细粒度更新可变状态的接口,这些接口的容错方式就是通过集群中节点间的数据移动和复制,或日志更新集群间各个节点 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.12950478164763524
而这两种方法对于数据密集型负载来说开销太大,原因在于:这些方法需要在集群网点节点间复制大量数据,而网络带宽又远远小于内存带宽,这将导致大量存储开销 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.12309149097933274
Spark 采用了基于粗粒度的转化的接口,这种转化操作在计算过程中形成一种有向无环图(DAG),称为“血统(lineage)”,“血统”实质上是建立一种数据间转换的关系,而不是数据本身,因此当出现系统故障时,便可通过这个“血统”提供的信息,计算丢失的数据,即以计算换取数据移动和复制,这样即可大量节省容错所带来的开销 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.13888888888888892
同样,在Nectar[82]系统中,同样采用了“血统”容错,只是在特定编程框架下才采用“血统”容错,在传统的情形下,还是通过复制数据来解决容错问题 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.08606629658238704
Tachyon[90]也采用了“血统”,使得在运用内存缓存容错时,数据恢复得到Memory 访问速度级别的加速 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.05892556509887897
另外,虽然“血统”通过计算解决了不用复制和移动数据的问题,但是当“血统”的有向无环图很大时,则需要较长的时间来计算恢复数据 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.10050378152592121
在这种情况下,Spark 和Tachyon 都采用了检查点机制来解决“血统”链很长所带的计算开销问题 VS 继Ｈｅｂｂ 学习规则之后，神经元的有监督Ｄｅｌｔａ学习规则被 提出，用以解决在输入输出已知的情况下神经元权 值的学习问题 = 0.10300524052492097
目前,容错处理主要有数据移动和复制、日志机制、分布式锁、快照、检查点机制等等 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.06537204504606135
但是正如文献[80]所述,由于带宽限制,数据复制等数据恢复机制带来巨大开销,而基于“血统”容错的方法能很好地和内存计算在处理速度上相协调,因此,这可能将成为内存计算未来主要的容错方法 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.07856742013183861
(2) 同步分布式内存处理系统主要处理机器学习、图算法、科学计算等问题,此类问题在并行计算的同时,往往涉及到结构或逻辑上的依赖关系,而并行处理的各个步骤在到达稳定点的时间不同,因此需要在并行进行的计算步之间进行同步控制,以保证结果的正确性 VS Ｈｅｂｂ规则隶属于无监督学习算法的范畴，其 主要思想是根据两个神经元的激发状态来调整其连 接关系，以此实现对简单神经活动的模拟 = 0.10476454436543672
常见的同步方式有同步计算、异步计算和混合方式 = 0.0
目前,大多数内存计算系统采用BSP[91]同步,部分系统采用异步机制 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.04380028798784028
Spark 和Pregel 都采用BSP 同步机制,而基于内存计算的分布式内存共享图处理系统PowerGraph[92]则采用BSP 同步和异步两种方式 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.03150094602699077
Trinity[93]同样采用BSP 同步和异步两种方式 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.02998800719520336
同步方式可确保计算的确定性,易于设计、编程和测试 VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.11973686801784995
但是由于每个计算步的处理时间不同,最慢的计算将严重影响整体收敛速度 = 0.0
异步处理模式的优势在于可加速计算的收敛速度,但其编程复杂,不便于调试和测试,不能保证更新一致性,且结果不确定 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.07412493166611013
两者各有优缺点,如何根据不同应用系统类型,取两者优点设计出高效且易于编程的同步机制越来越重要 VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.09962709627734359
(3) 内存分配与管理内存计算的核心资源为内存,因而内存分配与管理显得尤为重要 = 0.0
如何在内存中安排数据存储方式,使数据访问更为高效且内存资源充分利用,这是首要解决的问题 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.21629522817435007
在实际的应用实例中,应用类型不同,内存数据安排也不尽相同,如Spark 系统采用“键-值”存储方式,Piccolo 采用内存表格方式及“键-值”两种方式,Pregel 采用了BigTable 的内存组织方式 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.16037507477489607
另外,内存容量总是有限的,如何高效利用有限的内存资源处理海量文件又是一个挑战 = 0.0
Spark 将内存数据抽象成RDD,然后在内存不足时,利用“最近最少使用”(LRU)内存替换策略协调内存资源 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.049147318718299055
同时,为了更灵活地分罗乐 等:内存计算技术研究综述 2153配内存资源,Spark 可以通过所谓的RDD“持续存储优先权”给用户一定的管理权限 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.051434449987363975
总之,在此类内存计算当中,同样存在内存替换策略的选择 = 0.0
因此,如何根据具体的编程框架的优缺点选择高效的内存替换策略,有待进一步研究 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.1421338109037403
(4) 网络瓶颈谷歌的一份报告[94]显示:从内存读取数据比从本地磁盘或集群网络中其他主机读取数据快两个数量级,而磁盘和集群网络读取速度又处于同一数量级 VS Ｍ－Ｐ模型是对 生物神经元信息处理模式的数学简化，后续的神经 网络研究工作都是以它为基础的 = 0.11821656093586509
因此,在本地处理数据时,数据存在于内存当中,CPU 很少停下来等待磁盘I/O,这样,内存计算解决了磁盘I/O 瓶颈 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.18344984642633572
但是,如果涉及到节点间通信或数据传输,则要比直接在内存读写慢两个数量级,内存计算必然面临一定的效率损失 VS 感知机参数的学 习是基于经验损失函数最小化的，旨在最小化误分类 点到决策平面的距离 = 0.05407380704358752
因此,在分布式集群内存计算当中,网络瓶颈成为主要瓶颈 VS Ｍ－Ｐ模型是对 生物神经元信息处理模式的数学简化，后续的神经 网络研究工作都是以它为基础的 = 0.0890870806374748
如何减少节点间的通信和数据传输开销,提高内存计算效率,这又是一个挑战 = 0.0
3 新型混合内存结构的内存计算近几年, 新兴的非易失性随机存储介质(non-volatile memory, 简称NVM) 快速发展, 如铁电存储器(ferroelectric random accessmemory,简称FeRAM)[95]、相变存储器(phase change memory,简称PCM)[96?98]、电阻存储器(resistive randomaccess memory,简称RRAM)[99]等,其性能接近DRAM,但容量远远大于DRAM,同时,能耗和价格远远低于DRAM VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.024441185836892216
这为新型的内存体系结构提供了良好的硬件保障 VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.07332355751067665
因此,基于新型存储器件和传统DRAM 的新型混合内存体系在大幅提升内存容量,降低成本的同时,其访问速度与DRAM 相当 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.05270462766947299
在众多的非易失性随机存储介质中,PCM 凭借其非易失性、非破坏性读、读完无须回写、写操作无须先擦除、存储密度高等特性,逐渐成为大规模内存系统中颇具潜力的DRAM 替代品[98] VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.12274328238644314
在硬件体系结构方面,研究者围绕PCM 和DRAM 的混合方案开展了很多研究,国内外学术界出现的混合内存结构大概分为3类 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.11111111111111113
1 线性统一编址混合内存这种混合内存结构由PCM 和DRAM 构成[100,101] VS 以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 = 0.05590169943749474
通过软件和硬件技术,避免了PCM 较短的写寿命和较高的写能耗的缺点,充分发挥了PCM 在读数据和存储数据方面低功耗、非易失性和DRAM 在写数据时低功耗及超长的写寿命的特性 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.12598815766974242
实验表明:相比DRAM,PDRAM 的能耗节省达到37% VS 受启发于巴浦洛夫的条件反 射实验，Ｈｅｂｂ认为如果两个神经元在同一时刻被 激发，则它们之间的联系应该被强化，基于此所定义 的Ｈｅｂｂ学习规则如下所示： ｗｉｊ（ｔ＋１）＝ｗｉｊ（ｔ）＋αｙｊ（ｔ）ｙｉ（ｔ） （２） 其中，ｗｉｊ（ｔ＋１）和ｗｉｊ（ｔ）分别表示在ｔ＋１和ｔ 时刻时，神经元ｊ到神经元ｉ之间的连接强度，而ｙｉ 和ｙｊ 则为神经元ｉ和ｊ 的输出 = 0.03464794643376185
在PDRAM 中,PCM 和DRAM 处于同等地位,无主次之分,对两者进行统一的线性编址,如图1 所示 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.08980265101338746
Fig = 0.0
1 Hybrid memory structure sharing a single physical address space图1 线性统一编址混合内存结构 VS 以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 = 0.05423261445466404
2 以DRAM 作为PCM 缓存的混合内存这种混合型内存体系结构是由DRAM 和PCM 共同组成,其中,PCM 作为主存,DRAM 作为PCM 的缓存,其主要目的是利用PCM 高容量以及DRAM 快速访问的特点,同时,避免了PCM 访问速度慢及DRAM 容量小 = 0.0
另外,文献[104]中通过3 种技术来减少PCM 上的写操作,以达到延长PCM 寿命的目的 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.06299407883487121
实验结果表明,这种内存系统较传统的内存容量提高了4 倍,页错误平均降低5 倍,PCM写操作次数降低了3 倍,可延长PCM 寿命3~9 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.024906774069335898
7 年 = 0.0
且当DRAM 容量为PCM 的3%时,PCM 较DRAM 的访问延迟得到了很多的弥补 VS 假设总共进行了ｎ次更新，则最终学习 到的ｗ 和ｂ 可表示为 ｗ＝Σ ｎ ｉ＝１ ａｉｙｉｘｉ （１２） ｂ＝Σ ｎ ｉ＝１ ａｉｙｉ （１３） 其中，ａｉ＝ｎｉη（ｎｉ为第ｉ 次时的累积更新次数） = 0.037062465833055065
图2 描述了这种内存系统与传统内存系统的区别 VS 神经元 ８期焦李成等：神经网络七十年：回顾与展望１６９９ 的输出ｙｉ 可以表示为如下形式： ｙｉ＝ｆ Σ ｎ ｊ＝１ ｗｉｊｘｊ－（ θｉ） （１） 　　该模型从逻辑功能器件的角度来描述神经元， 为神经网络的理论研究开辟了道路 = 0.048112522432468816
L1 cacheL2 cacheCPUMain memoryDRAM cachePCM memoryFig = 0.0
2 Hybrid memory structure using DRAM as cache图2 DRAM 缓存混合内存结构 VS 以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 = 0.05773502691896257
3 分层混合内存(hierarchical hybrid memory)文献[105]中提出一种名叫MN-MATE 的混合内存 VS Ｈｅｂｂ学习规则和Ｄｅｌｔａ学习规则都是针对单 个神经元而提出的，在神经元组成的网络中参数的 学习规则将会在后续述及 = 0.07987230638308716
这种内存是由PCM 和DRAM 构成,具有层次结构 VS 以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 = 0.09999999999999998
这种层次内存分为片上和片下两部分:片上内存由单独的DRAM 构成,因其内置于处理器因此具有较小的延迟 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.07744030926623664
片下部分则由PCM和DRAM 混合构成,两者共用同一个内存控制器且采用统一的线性编址 VS 感知机是二分类 的线性判别模型，旨在通过最小化误分类损失函数 来优化分类超平面，从而对新的实例实现准确预测 = 0.048112522432468816
如图3 所示,M1 为上片,M2 为下片,分别由PCM 和DRAM 通过线性编址组成 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.09962709627734359
同样,这种内存结构通过3 种技术发挥了PCM 的大容量低能耗以及DRAM 访问速度快的优点,提高内存性能和减少能耗 VS 以上为感知机学习 的原始形式，与之相对应的另一种结构是感知机学 习的对偶形式 = 0.0512989176042577
CacheM1 memoryCPULinera address spaceM2 memoryPCM memory DRAM cacheFig = 0.0
3 Hierarchical hybrid memory structure图3 分层混合内存结构另外,虽然诸如PCM 等新型存储介质和DRAM 构成的混合内存在硬件技术方向的研究已经进展不小,但是其相应的软件平台研究仍相对滞后 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.17507524381296344
现阶段想要利用新型混合内存处理实时大数据,仍然有很多工作做要 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.07453559924999299
罗乐 等:内存计算技术研究综述 2155虽然各种新型NVM 在容量、能耗方面比当前的DRAM 更具有优势,但是在访问速度、写寿命等方面仍然不及DRAM[96] = 0.0
比如:对于PCM 存在的缺点,文献[106]通过使用消除冗余位写入、行替换和段交换等方法降低损耗,可延长PCM 的使用寿命13 到22 年 VS １９５８年，Ｒｏｓｅｎｂｌａｔｔ等人成功研制出了代号为 Ｍａｒｋ　Ｉ的感知机（Ｐｅｒｃｅｐｔｒｏｎ），这是历史上首个将 神经网络的学习功能用于模式识别的装置，标志着 神经网路进入了新的发展阶段［２２］ = 0.03806934938134405
为了解决混合内存中PCM 写数据速度慢且写寿命较短的问题,文献[107,108]分别提出不同的内存管理技术,通过不同的预测写操作算法,让大多数写操作发生在DRAM 而非PCM,从而延长了PCM 的写寿命,并且可以隐藏PCM 写数据慢的缺点 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.07495316889958616
文献[109]不仅采用减少PCM 写次数的方法延长PCM 寿命,还采用了提高DRAM 缓存命中率的方法,从两个方面来优化混合内存结构的能耗和性能 VS 给定一组数据集Ｔ＝｛（ｘ１，ｙ１）， （ｘ２，ｙ２），…，（ｘｎ，ｙｎ）｝，假设超平面Ｓ 下误分类点的 集合为Ｍ ，则感知机学习的损失函数定义为 Ｌ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （６） 感知机学习算法通过最小化经验风险来优化参 数ｗ 和ｂ： ｍｉｎ ｗ，ｂＬ（ｗ，ｂ）＝－Σｘｉ∈Ｍ ｙｉ（ｗ·ｘｉ＋ｂ） （７） 优化过程采用随机梯度下降法，每次随机选取 一个误分类点使其梯度下降 = 0.047251419040486166
此外,针对PCM 等新型随机存储介质的缺点,还有众多研究从软硬件层面提出了很多的改进措施,也取得了不错的效果[98,110,111] VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.11111111111111113
在硬件体系结构方面,这种内存计算的主要革新表现在:在原来的DRAM 基础上,通过加入新型SCM 扩展成了不同类型的混合结构大内存,访问速度接近于DRAM,而容量远远大于DRAM VS 继感知机之后，许多新的学习型神经网络模型被提出，其中包括Ｗｉｄｒｏｗ等人设计的自适应线性元件Ａｄａｌｉｎｅ［２３］和由Ｓｔｅｉｎｂｕｃｈ等人设计的被称为学习矩阵的二进制联想网络及其硬件实现［２４］ = 0.03225806451612904
在这种内存计算模式中,SCM 不仅可作为内存,而且相当于传统磁盘,计算可以直接发生在SCM 上,避免了传统的内存—磁盘数据访问,CPU 直接从内存读取数据,进行计算分析 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.09901475429766744
同时,这种新型内存架构也带来了一系列问题:(1) 各种新型内存与DRAM 存在访存速度、能耗、读写寿命等方面差异,如何协调这些差异,使整个内存处理效率较高,能耗较少?(2) 随着众核处理器性能越来越高,异构内存容量增大,如何处理大内存与众核处理器间不断加大的带宽鸿沟?(3) 内存较原来更大,如何应对大内存带来的高能耗问题? = 0.0
相对于硬件体系结构,在软件方面,这种内存计算的相关研究滞后很多 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.15713484026367722
因为作为一种新兴的混合内存结构,它涉及到体系结构、操作系统、编程模型方面的诸多问题 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.10830607221477646
(1) 体系结构由于新型内存替代了磁盘可以长期存储数据,那么传统的磁盘数据访问局部性将被内存访问局部性取代而成为性能优化的主要目标 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.12909944487358058
并且,传统的磁盘数据预取、缓存以及替换策略无法直接迁移到非一致缓存访问以及非一致内存访问环境中 VS 通过这 样的调节过程，神经元会将输入和输出之间的正确 映射关系存储在权值中，从而具备了对数据的表示 能力 = 0.09622504486493764
(2) 操作系统由于新型混合结构的出现,操作系统需要统一管理多种异构资源,实现高效的、透明的、可靠的内存访问与管理策略 = 0.0
另外,由于新型内存容量增大,可用地址空间随之增大,致使操作系统内存管理开销增大 VS 感知机的假设空间是定义在特征空间中的所有 线性分类器，所得的超平面把特征空间划分为两部分，位于两侧的点分别为正负两类 = 0.08304547985373997
所以,为了提高操作系统对内存管理效率,需要研究新的编址策略和访问方式 VS 以上先驱者所做的研究 工作为后来神经计算的出现铺平了道路，激发了许 多学者对这一领域的继续探索和研究 = 0.13608276348795434
(3) 编程模型新型混合内存解决了磁盘I/O 瓶颈问题,因此在内存计算当中,传统的编程模型对数据传输不再占大量处理时间,而数据处理将占据大量处理时间 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.1311651671567906
此外,由于新型非易失性内存的I/O 延迟远小于磁盘,传统模型中磁盘与内存的数据一致性等问题不复存在,编程模型也因此有相应改变 VS １９４３年，神经元 的Ｍ－Ｐ模型（如图１所示）在论文《神经活动中所蕴 含思想的逻辑活动》中被首次提出［１９］，创建该模型 的是来自美国的心理学家ＭｃＣｕｌｌｏｃｈ以及另一位 数学家Ｐｉｔｔｓ = 0.13340746919301402
Avg similar = 0.07690624060044458
