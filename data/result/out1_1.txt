１　引　言实现人工智能是人类长期以来一直追求的梦想 VS １　引　言实现人工智能是人类长期以来一直追求的梦想 = 1.0000000000000002
虽然计算机技术在过去几十年里取得了长足的发展，但是实现真正意义上的机器智能至今仍然困难重重 VS 虽然计算机技术在过去几十年里取得了长足的发展，但是实现真正意义上的机器智能至今仍然困难重重 = 0.9999999999999998
伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 VS 伴随着神经解剖学的发展，观测大脑微观结构的技术手段日益丰富，人类对大脑组织的形态、结构与活动的认识越来越深入，人脑信息处理的奥秘也正在被逐步揭示 = 1.0
如何借助神经科学、脑科学与认知科学的研究成果，研究大脑信息表征、转换机理和学习规则，建立模拟大脑信息处理过程的智能计算模型，最终使机器掌握人类的认知规律，是“类脑智能”的研究目标 VS 如何借助神经科学、脑科学与认知科学的研究成果，研究大脑信息表征、转换机理和学习规则，建立模拟大脑信息处理过程的智能计算模型，最终使机器掌握人类的认知规律，是“类脑智能”的研究目标 = 1.0000000000000002
近年来，类脑智能已成为世界各国研究和角逐的热点 VS 近年来，类脑智能已成为世界各国研究和角逐的热点 = 0.9999999999999998
继美国及欧盟各国之后，我国经过两三年筹备的“中国脑科学计划”在２０１５年浮出水面，科技部正在规划“脑科学与类脑研究”的重大专项，北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究，大规模“类脑智能”的研究正蓄势待发 VS 继美国及欧盟各国之后，我国经过两三年筹备的“中国脑科学计划”在２０１５年浮出水面，科技部正在规划“脑科学与类脑研究”的重大专项，北京大学、清华大学、复旦大学等高校和中国科学院等研究机构也发力推动神经与类脑计算的相关研究，大规模“类脑智能”的研究正蓄势待发 = 0.9999999999999999
类脑智能是涉及计算科学、认知科学、神经科学与脑科学的交叉前沿方向 VS 类脑智能是涉及计算科学、认知科学、神经科学与脑科学的交叉前沿方向 = 0.9999999999999999
类脑智能的实现离不开大脑神经系统的研究 VS 类脑智能的实现离不开大脑神经系统的研究 = 0.9999999999999999
众所周知，人脑是由几十多亿个高度互联的神经元组成的复杂生物网络，也是人类分析、联想、记忆和逻辑推理等能力的来源 VS 众所周知，人脑是由几十多亿个高度互联的神经元组成的复杂生物网络，也是人类分析、联想、记忆和逻辑推理等能力的来源 = 1.0
神经元之间通过突触连接以相互传递信息，连接的方式和强度随着学习发生改变，从而将学习到的知识进行存储 VS 神经元之间通过突触连接以相互传递信息，连接的方式和强度随着学习发生改变，从而将学习到的知识进行存储 = 0.9999999999999998
模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 VS 模拟人脑中信息存储和处理的基本单元－神经元而组成的人工神经网络模型具有自学习与自组织等智能行为，能够使机器具有一定程度上的智能水平 = 0.9999999999999998
神经网络的计算结构和学习规则遵照生物神经网络设计，在数字计算机中，神经细胞接收周围细胞的刺激并产生相应输出信号的过程可以用“线性加权和”及“函数映射”的方式来模拟，而网络结构和权值调整的过程用优化学习算法实现 VS 神经网络的计算结构和学习规则遵照生物神经网络设计，在数字计算机中，神经细胞接收周围细胞的刺激并产生相应输出信号的过程可以用“线性加权和”及“函数映射”的方式来模拟，而网络结构和权值调整的过程用优化学习算法实现 = 1.0
按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 VS 按照该方式建立的这种仿生智能计算模型虽然不能和生物神经网络完全等价和媲美，但已经在某些方面取得了优越的性能 = 0.9999999999999999
从２０世纪４０年代的Ｍ－Ｐ神经元和Ｈｅｂｂ学习规则，到５０年代的Ｈｏｄｙｋｉｎ－Ｈｕｘｌｅｙ方程、感知器模型与自适应滤波器，再到６０年代的自组织映射网络、神经认知机、自适应共振网络，许多神经计算模型都发展成为信号处理、计算机视觉、自然语言处理与优化计算等领域的经典方法，为该领域带来了里程碑式的影响 VS 从２０世纪４０年代的Ｍ－Ｐ神经元和Ｈｅｂｂ学习规则，到５０年代的Ｈｏｄｙｋｉｎ－Ｈｕｘｌｅｙ方程、感知器模型与自适应滤波器，再到６０年代的自组织映射网络、神经认知机、自适应共振网络，许多神经计算模型都发展成为信号处理、计算机视觉、自然语言处理与优化计算等领域的经典方法，为该领域带来了里程碑式的影响 = 1.0
目前神经网络已经发展了上百种模型，在诸如手写体识别［１－２］、图像标注［３］、语义理解［４－６］和语音识别［７－９］等技术领域取得了非常成功的应用 VS 目前神经网络已经发展了上百种模型，在诸如手写体识别［１－２］、图像标注［３］、语义理解［４－６］和语音识别［７－９］等技术领域取得了非常成功的应用 = 0.9999999999999998
从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 VS 从数据容量和处理速度来看，目前大多数神经网络是生物网络的简化形式，在应对海量数据和处理复杂任务时显得力不从心 = 1.0
例如，人脑被证明可以在没有导师监督的情况下主动地完成学习任务，仅凭借传统的浅层神经网络是无法实现这一点的 VS 例如，人脑被证明可以在没有导师监督的情况下主动地完成学习任务，仅凭借传统的浅层神经网络是无法实现这一点的 = 1.0000000000000002
最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 VS 最近发展起来的深层神经网络就是一种类脑智能软件系统，它使得人工智能的研究进入了一个新阶段 = 1.0000000000000002
深层神经网络通过增加网络的层数来模拟人脑复杂的层次化认知规律，以使机器获得“抽象概念”的能力，在无监督特征学习方面具有更强的能力 VS 深层神经网络通过增加网络的层数来模拟人脑复杂的层次化认知规律，以使机器获得“抽象概念”的能力，在无监督特征学习方面具有更强的能力 = 1.0000000000000002
然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 VS 然而，受到计算平台和学习算法的限制，对深层神经网络的研究曾一度消弭 = 1.0
２００６年，Ｈｉｎｔｏｎ在《科学》上提出了一种面向复杂通用学习任务的深层神经网络，指出具有大量隐层的网络具有优异的特征学习能力，而网络的训练可以采用“逐层初始化”与“反向微调”技术解决 VS ２００６年，Ｈｉｎｔｏｎ在《科学》上提出了一种面向复杂通用学习任务的深层神经网络，指出具有大量隐层的网络具有优异的特征学习能力，而网络的训练可以采用“逐层初始化”与“反向微调”技术解决 = 1.0
人类借助神经网络找到了处理“抽象概念”的方法，神经网络的研究又进入了一个崭新的时代［１０－１２］，深度学习的概念开始被提出 VS 人类借助神经网络找到了处理“抽象概念”的方法，神经网络的研究又进入了一个崭新的时代［１０－１２］，深度学习的概念开始被提出 = 0.9999999999999998
深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 VS 深度学习兴起的背景是计算能力的提高与大数据时代的来临，其核心理念是通过增加网络的层数来让机器自动地从数据中进行学习 = 1.0000000000000002
深层神经网络能够获得巨大成功与其对应在训练算法上所取得的突破性进展是密不可分的 VS 深层神经网络能够获得巨大成功与其对应在训练算法上所取得的突破性进展是密不可分的 = 1.0
传统的反向传播算法（Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ）随着传递层数的增加，残差会越来越小，出现所谓的“梯度扩散”（Ｇｒａｄｉｅｎｔ　Ｄｉｆｆｕｓｉｏｎ）现象，故而不适于深层网络的训练 VS 传统的反向传播算法（Ｂａｃｋ　Ｐｒｏｐａｇａｔｉｏｎ）随着传递层数的增加，残差会越来越小，出现所谓的“梯度扩散”（Ｇｒａｄｉｅｎｔ　Ｄｉｆｆｕｓｉｏｎ）现象，故而不适于深层网络的训练 = 1.0
深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 VS 深度学习模型中的受限玻尔兹曼机（Ｒｅｓｔｒｉｃｔｅｄ　Ｂｏｌｔｚｍａｎｎ　Ｍａｃｈｉｎｅｓ）和自编码器（Ａｕｔｏ－Ｅｎｃｏｄｅｒ）采用了“自下而上的无监督学习”和“自顶向下的监督学习”策略来实现对网络的“预训练”和“微调”，可使学习算法收敛到较为理想的解上，而当前使用更为广泛的卷积神经网络（Ｃｏｎｖｏｌｕｔｉｏｎａｌ　Ｎｅｕｒａｌ　Ｎｅｔｗｏｒｋｓ）则采用局部感受野、权值共享和时空亚采样的思想，显著地减少了网络中自由参数的个数，并且使得采用反向传播来进行网络的并行学习成为可能 = 1.0000000000000002
除了以上优势外，深度学习最具吸引力的地方还在于能凭借无标签的数据来进行学习，而不需要依赖于监督信息的支撑［１３］ VS 除了以上优势外，深度学习最具吸引力的地方还在于能凭借无标签的数据来进行学习，而不需要依赖于监督信息的支撑［１３］ = 1.0000000000000002
现实世界的很多问题中，对数据的标记通常是耗时耗力甚至是不可行的，无监督学习可以自动抽取出抽象的高层属性和特征，是解决样本标记难问题的一个重大突破 VS 现实世界的很多问题中，对数据的标记通常是耗时耗力甚至是不可行的，无监督学习可以自动抽取出抽象的高层属性和特征，是解决样本标记难问题的一个重大突破 = 1.0000000000000002
深度学习的成功引起了包括产业界和学术界在内的诸多人士的关注，其影响力甚至上升到了国家战略层面 VS 深度学习的成功引起了包括产业界和学术界在内的诸多人士的关注，其影响力甚至上升到了国家战略层面 = 1.0
２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 VS ２０１２年６月，《纽约时报》披露了Ｇｏｏｇｌｅ　Ｂｒａｉｎ项目，该项目拟计划在包含１６　０００个中央处理单元的分布式并行计算平台上构建一种被称之为“深度神经网络”的类脑学习模型，其主要负责人为机器学习界的泰斗、来自斯坦福大学的Ｎｇ教授和Ｇｏｏｇｌｅ软件架构天才、大型并发编程框架Ｍａｐ　Ｒｅｄｕｃｅ的作者Ｊｅｆｆ　Ｄｅａｎ；２０１２年１０月，在天津举行的“２１世纪的计算大会”上，微软首席研究官Ｒｉｃｋ　Ｒａｓｈｉｄ展示了一套全自动同声传译系统，演讲者的英文能够被实时、流畅地转换成与之对应的、音色相近的中文，其背后的关键技术深度神经网络也逐渐被人们所知 = 1.0000000000000002
２０１３年１月，作为百度公司创始人兼ＣＥＯ的李彦宏在其年会上宣布了成立百度研究院的计划，并且强调首当其冲的就是组建“深度学习研究所” VS ２０１３年１月，作为百度公司创始人兼ＣＥＯ的李彦宏在其年会上宣布了成立百度研究院的计划，并且强调首当其冲的就是组建“深度学习研究所” = 1.0000000000000002
在２０１５年３月９日的两会期间，李彦宏又提议设立“中国大脑”计划的提案，与２０１３年１月和２０１３年４月的“欧盟大脑计划”和“美国大脑计划”相呼应 VS 在２０１５年３月９日的两会期间，李彦宏又提议设立“中国大脑”计划的提案，与２０１３年１月和２０１３年４月的“欧盟大脑计划”和“美国大脑计划”相呼应 = 0.9999999999999998
２０１５年３月，阿里巴巴公司的创始人马云通过支付宝的“刷脸支付”功能，在德国举行的ＩＴ 博览会上成功购得了一款汉诺威纪念邮票 VS ２０１５年３月，阿里巴巴公司的创始人马云通过支付宝的“刷脸支付”功能，在德国举行的ＩＴ 博览会上成功购得了一款汉诺威纪念邮票 = 1.0
这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” VS 这一人脸识别技术在商业领域的应用雏形所采用的是基于神经网络的技术，其网络训练所使用的正是“深度学习算法” = 1.0
在学术界，以Ｈｉｎｔｏｎ、ＬｅＣｕｎ、Ｂｅｎｇｉｏ和Ｎｇ等为代表的神经网络大师们不断将深度学习的研究推向新的高峰，对包括计算机视觉、自然语言处理和机器学习在内的诸多领域带来了深远的影响［１４］ VS 在学术界，以Ｈｉｎｔｏｎ、ＬｅＣｕｎ、Ｂｅｎｇｉｏ和Ｎｇ等为代表的神经网络大师们不断将深度学习的研究推向新的高峰，对包括计算机视觉、自然语言处理和机器学习在内的诸多领域带来了深远的影响［１４］ = 1.0000000000000002
自２００６年深度学习出现以来，关于深度学习理论和应用方面的研究文献在国际知名期刊和会议上不断涌现，如《自然》、《科学》、ＰＡＭＩ、ＮＩＰＳ、ＣＶＰＲ、ＩＣＭＬ等 VS 自２００６年深度学习出现以来，关于深度学习理论和应用方面的研究文献在国际知名期刊和会议上不断涌现，如《自然》、《科学》、ＰＡＭＩ、ＮＩＰＳ、ＣＶＰＲ、ＩＣＭＬ等 = 0.9999999999999998
同时，由Ｂｅｎｇｉｏ等人编写的第一本关于深度学习的专著“Ｄｅｅｐ　Ｌｅａｒｎｉｎｇ”也即将由ＭＩＴ出版社出版 VS 同时，由Ｂｅｎｇｉｏ等人编写的第一本关于深度学习的专著“Ｄｅｅｐ　Ｌｅａｒｎｉｎｇ”也即将由ＭＩＴ出版社出版 = 1.0000000000000002
包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 VS 包括斯坦福大学、卡内基梅隆大学、纽约大学、多伦多大学等在内的机构都提供了深度学习的公开课程，并公开了实验数据和源代码，为深度学习的进一步发展做出了贡献 = 1.0
在国内，深度学习也受到了学术界的广泛关注，但目前主要是以深度学习的应用研究为主，在理论方面的工作相对较少 VS 在国内，深度学习也受到了学术界的广泛关注，但目前主要是以深度学习的应用研究为主，在理论方面的工作相对较少 = 1.0
以北京大学、浙江大学、上海交通大学、哈尔滨工业大学和西安电子科技大学等为代表的研究人员将深度学习算法应用到遥感图像分类［１５］、多媒体检索［１６］、交通流预测［１７］和盲图像质量评价［１８］等领域，取得了较传统方法更优的效果 VS 以北京大学、浙江大学、上海交通大学、哈尔滨工业大学和西安电子科技大学等为代表的研究人员将深度学习算法应用到遥感图像分类［１５］、多媒体检索［１６］、交通流预测［１７］和盲图像质量评价［１８］等领域，取得了较传统方法更优的效果 = 1.0
本文将以神经网络的理论和应用为主线，回顾神经网络在过去七十多年的发展历程及主要成就，重点对新近发展起来的深度学习进行阐述和讨论，并对未来的研究方向做出展望 VS 本文将以神经网络的理论和应用为主线，回顾神经网络在过去七十多年的发展历程及主要成就，重点对新近发展起来的深度学习进行阐述和讨论，并对未来的研究方向做出展望 = 1.0000000000000002
Avg similar = 1.0
